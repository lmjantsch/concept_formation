{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503273e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import json\n",
    "import sys\n",
    "import gc\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, PretrainedConfig, AutoConfig\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "from data.utils import get_entity_idx, get_prompts\n",
    "from mi_toolbox.utils.data_types import DataDict\n",
    "from mi_toolbox.utils.collate import TensorCollator\n",
    "from mi_toolbox.transformer_caching import caching_wrapper, decompose_attention_to_neuron, decompose_glu_to_neuron, TransformerCache\n",
    "from mi_toolbox.causal_tracing import get_mass_mean_vectors\n",
    "from mi_toolbox.contribution_tracing import get_dot_prod_contribution, get_top_x_contribution_values\n",
    "\n",
    "\n",
    "model_ids = [\"Qwen/Qwen3-4B\"]#, \"Qwen/Qwen3-8B\", \"Qwen/Qwen3-14B\"]  # \"Qwen/Qwen3-0.6B\",\"Qwen/Qwen3-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ids[0])\n",
    "config = AutoConfig.from_pretrained(model_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2f87a",
   "metadata": {},
   "source": [
    "### Get Concept Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3abb88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 0\n",
    "num_layers = 10\n",
    "data_dir = f'/raid/dacslab/CONCEPT_FORMATION/homograph_small/Qwen_Qwen3-4B/{sample_id}/'\n",
    "\n",
    "#TODO incorporate into TransformerCache -> also init from pretrained (AutoConfig)\n",
    "data_keys = ['emb'] + [f\"{layer}.{module}\" for layer in range(num_layers) for module in ['mid', 'post']]\n",
    "data_dir_files = os.listdir(data_dir)\n",
    "target_token_res_states = {}\n",
    "for key in data_keys:\n",
    "    if f\"{key}.json\" in data_dir_files:\n",
    "        file_path = os.path.join(data_dir, f\"{key}.json\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            target_token_res_states[key] = json.load(f)\n",
    "    if f\"{key}.safetensors\" in data_dir_files:\n",
    "        file_path  = os.path.join(data_dir, f\"{key}.safetensors\")\n",
    "        target_token_res_states[key] = torch.stack(torch.load(file_path))\n",
    "\n",
    "target_token_res_states = TransformerCache.from_dict(target_token_res_states, model_config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf6b16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "concept_vectors = {}\n",
    "no_layers = 10\n",
    "num_homographs = 1\n",
    "num_examples = 5\n",
    "\n",
    "concept_vectors = {}\n",
    "\n",
    "emb_pairs = target_token_res_states[f'emb'].view(num_homographs, 2, num_examples, -1)\n",
    "concept_vectors['emb'] = get_mass_mean_vectors(emb_pairs)\n",
    "\n",
    "for layer in range(no_layers):\n",
    "    mid_pairs = target_token_res_states[f'{layer}.mid'].view(num_homographs, 2, num_examples, -1)\n",
    "    post_pairs = target_token_res_states[f'{layer}.post'].view(num_homographs, 2, num_examples, -1)\n",
    "    \n",
    "    concept_vectors[f'{layer}.mid'] = get_mass_mean_vectors(mid_pairs)\n",
    "    concept_vectors[f'{layer}.post'] = get_mass_mean_vectors(post_pairs)\n",
    "\n",
    "del target_token_res_states\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac8a81",
   "metadata": {},
   "source": [
    "### Get direct contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf39d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys = ['ent_pos_idx', 'emb'] + [f\"{layer}.{module}\" for layer in range(num_layers) for module in ['d_attn', 'd_mlp']]\n",
    "data_dir_files = os.listdir(data_dir)\n",
    "model_cache = {}\n",
    "for key in data_keys:\n",
    "    if f\"{key}.json\" in data_dir_files:\n",
    "        file_path = os.path.join(data_dir, f\"{key}.json\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            model_cache[key] = json.load(f)\n",
    "    if f\"{key}.safetensors\" in data_dir_files:\n",
    "        file_path  = os.path.join(data_dir, f\"{key}.safetensors\")\n",
    "        model_cache[key] = torch.load(file_path)\n",
    "\n",
    "model_cache = TransformerCache.from_dict(model_cache, model_config=config)\n",
    "data_keys = [f\"{layer}.{module}\" for layer in range(num_layers) for module in ['d_attn', 'd_mlp']]\n",
    "model_cache.map(\n",
    "    fn = lambda row_id, row: {key:row[key][row['ent_pos_idx']] for key in data_keys}\n",
    ")\n",
    "model_cache.stack_tensors(padding=True)\n",
    "\n",
    "num_examples = model_cache.length\n",
    "num_tokens = model_cache['0.d_attn'].size(1)\n",
    "target_token_pos = model_cache['ent_pos_idx']\n",
    "parts = [model_cache['emb'][:, None]]\n",
    "\n",
    "for layer in range(no_layers):\n",
    "    parts.append(model_cache[f'{layer}.d_attn'].flatten(1, -2))\n",
    "    parts.append(model_cache[f\"{layer}.d_mlp\"])\n",
    "parts = torch.concat(parts, dim=1).transpose(0,1)\n",
    "\n",
    "del model_cache\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_list(llm, target_layer, num_tokens):\n",
    "    num_attn_heads, attn_head_dim, num_mlp_heads = llm.config.num_attention_heads, llm.config.head_dim, llm.config.intermediate_size\n",
    "    module_list = ['emb']\n",
    "    for layer_id in range(target_layer + 1):\n",
    "        module_list.extend([f\"{layer_id}.attn.{token_pos}.{head_pos}\" for token_pos in range(num_tokens) for head_pos in range(num_attn_heads * attn_head_dim)])\n",
    "        module_list.extend([f\"{layer_id}.mlp.-1.{head_pos}\" for head_pos in range(num_mlp_heads)])\n",
    "    \n",
    "    return module_list\n",
    "\n",
    "def get_first_module_group_id_lookup(module_list):\n",
    "    first_module_group_id_lookup = {}\n",
    "    prev_layer, prev_module_type = 0, ''\n",
    "    for i, module in enumerate(module_list):\n",
    "        if module == 'emb': continue\n",
    "        \n",
    "        curr_layer, curr_module_type, _, _ = module.split('.')\n",
    "        if not first_module_group_id_lookup or \\\n",
    "            curr_layer != prev_layer or curr_module_type != prev_module_type:\n",
    "            first_module_group_id_lookup[f\"{curr_layer}.{curr_module_type}\"] = i\n",
    "        \n",
    "        prev_layer, prev_module_type = curr_layer, curr_module_type\n",
    "        \n",
    "    return first_module_group_id_lookup\n",
    "\n",
    "def get_initial_tasks(concept_vectors, target_module, target_layer, num_examples):\n",
    "    tasks = DataDict(length=num_examples)\n",
    "        \n",
    "    target_vectors = concept_vectors[target_module]\n",
    "    target_vectors = torch.flatten(target_vectors[:, None].repeat(1, 10, 1) * torch.tensor([[1]] * 5 + [[-1]] * 5), 0, 1)\n",
    "    tasks.attach('target_vectors', target_vectors, force= True)\n",
    "    \n",
    "    tasks.attach('target_sample_id', range(num_examples), force= True)\n",
    "    tasks.attach('target_contribution_factor', torch.tensor([1] * num_examples), force= True)\n",
    "    tasks.attach('target_layer', [target_layer] * num_examples, force= True)\n",
    "    tasks.attach('target_module', [target_module] * num_examples, force= True)\n",
    "    tasks.attach('depth', [[0]] * num_examples, force= True)\n",
    "\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea155c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1022e13adcc143e5baf969dab58e539a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_module = '9.post'\n",
    "target_layer = 10\n",
    "contribution_cache = []\n",
    "batch_size = 16\n",
    "\n",
    "llm = AutoModel.from_pretrained(\n",
    "    model_ids[0],\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto',\n",
    "    dtype=torch.bfloat16,\n",
    "    attn_implementation = 'eager'\n",
    ")\n",
    "llm.layers[target_layer:].to('meta')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "module_list = get_module_list(llm, target_layer, num_tokens)\n",
    "first_module_group_id_lookup = get_first_module_group_id_lookup(module_list)\n",
    "curr_tasks = get_initial_tasks(concept_vectors, target_module, target_layer, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e2326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1361"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_fn = TensorCollator()\n",
    "dl = DataLoader(curr_tasks, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "num_batches = len(dl)\n",
    "direct_contributions = DataDict()\n",
    "\n",
    "for batch in dl:\n",
    "    target_vectors = batch['target_vectors']\n",
    "    sample_id = batch['target_sample_id']\n",
    "    target_layer = batch['target_layer']\n",
    "    \n",
    "    contributions = get_dot_prod_contribution(parts = parts, whole= target_vectors).transpose(0, 1)\n",
    "    top_x_contributions = get_top_x_contribution_values(contributions, 0.9) \n",
    "\n",
    "    contribution_idx = [el.nonzero(as_tuple=True)[0] for el in top_x_contributions]\n",
    "    contribution_values = [top_x_contributions[i][idx] for i, idx in enumerate(contribution_idx)]\n",
    "    contribution_modules = [[module_list[i] for i in idx] for idx in contribution_idx]\n",
    "    \n",
    "    direct_contributions.extend({\n",
    "        'contribution_idx': contribution_idx,\n",
    "        'contribution_values':contribution_values,\n",
    "        'contribution_modules':contribution_modules,\n",
    "        'target_sample_id': sample_id,\n",
    "        'target_token_pos': target_token_pos,\n",
    "        'target_module': batch['target_module'],\n",
    "        'depth': batch['depth'],\n",
    "        })\n",
    "    \n",
    "del parts\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d78ab5",
   "metadata": {},
   "source": [
    "### Get indirect contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4596e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([343041, 10, 2560])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_target_vectors_and_postions(llm, cache_item, model_cache):\n",
    "    contribution_modules = cache_item['contribution_modules']\n",
    "    contribution_values = cache_item['contribution_values']\n",
    "    sample_id = cache_item['target_sample_id']\n",
    "    source_token_pos = cache_item['target_token_pos']\n",
    "    eps = 1e-7\n",
    "\n",
    "    num_attn_heads = llm.config.num_attention_heads\n",
    "    num_k_v_heads = llm.config.num_key_value_heads\n",
    "    num_head_groups = num_attn_heads // num_k_v_heads\n",
    "    head_dim = llm.config.head_dim\n",
    "    hidden_dim = llm.config.hidden_size\n",
    "\n",
    "    new_target_vectors = []\n",
    "    new_target_token_pos = []\n",
    "    new_target_module = []\n",
    "    new_target_layer = []\n",
    "    new_target_contribution_factor = []\n",
    "    valid_contribution_idx = []\n",
    "\n",
    "    for i,( module_desc, cont_value) in enumerate(zip(contribution_modules, contribution_values)):\n",
    "        if module_desc == 'emb':continue\n",
    "        \n",
    "        layer_id, module_type, target_token_pos, head_pos = module_desc.split('.')\n",
    "        layer_id, head_pos, target_token_pos = int(layer_id), int(head_pos), int(target_token_pos)\n",
    "        \n",
    "        if target_token_pos == -1:\n",
    "            target_token_pos = source_token_pos\n",
    "        \n",
    "        new_target_token_pos.append(target_token_pos)\n",
    "        new_target_module.append(f\"{layer_id}.{module_type}\")\n",
    "        new_target_layer.append(layer_id)\n",
    "        valid_contribution_idx.append(i)\n",
    "        \n",
    "        if module_type == 'attn':\n",
    "            v_proj_W = llm.layers[layer_id].self_attn.v_proj.weight.data.view(num_k_v_heads, head_dim, hidden_dim)[head_pos//head_dim//num_head_groups, head_pos % head_dim]\n",
    "            norm = llm.layers[layer_id].input_layernorm.weight.data\n",
    "            \n",
    "            v_proj = model_cache[f\"{layer_id}.v_proj\"][sample_id, target_token_pos].view(num_k_v_heads, head_dim)[head_pos//head_dim//num_head_groups, head_pos % head_dim]\n",
    "            norm_var = model_cache[f\"{layer_id}.attn_norm_var\"][sample_id, source_token_pos] \n",
    "            \n",
    "            target_vector = v_proj_W * norm / norm_var.cuda()\n",
    "            target_contribution_factor = (cont_value / v_proj.cuda())\n",
    "            new_target_contribution_factor.append(target_contribution_factor.cpu())\n",
    "            \n",
    "        elif module_type == 'mlp':\n",
    "            up_proj_W = llm.layers[layer_id].mlp.up_proj.weight.data[head_pos]\n",
    "            norm = llm.layers[layer_id].post_attention_layernorm.weight.data\n",
    "        \n",
    "            up_proj =  model_cache[f\"{layer_id}.up_proj\"][sample_id, target_token_pos][head_pos]\n",
    "            norm_var = model_cache[f\"{layer_id}.mlp_norm_var\"][sample_id, source_token_pos]\n",
    "            \n",
    "            target_vector = up_proj_W * norm / norm_var.cuda()\n",
    "            target_contribution_factor = (cont_value / up_proj.cuda())\n",
    "            new_target_contribution_factor.append(target_contribution_factor.cpu())\n",
    "        \n",
    "        new_target_vectors.append(target_vector.cpu())\n",
    "        \n",
    "    new_target_vectors = torch.stack(new_target_vectors)\n",
    "\n",
    "    return new_target_vectors, new_target_token_pos, new_target_layer, new_target_module, new_target_contribution_factor, valid_contribution_idx\n",
    "\n",
    "@torch.no_grad()\n",
    "def trache_through_layer_norm(target: torch.Tensor, norm_W troch.Tensor, norm_var: torch.Tensor):\n",
    "    \n",
    "\n",
    "def get_current_tasks(tasks, contribution_cache):\n",
    "\n",
    "    for cache_item in contribution_cache.to_list():\n",
    "        if not cache_item['contribution_modules'] or torch.all(cache_item['contribution_idx'] == 0): continue\n",
    "        new_target_vectors, new_target_token_pos, new_target_layer, new_target_module, new_target_contribution_factor, valid_contribution_idx = get_target_vectors_and_postions(llm, cache_item, model_cache)\n",
    "\n",
    "        num_tasks = len(new_target_vectors)\n",
    "        source_idx = [cache_item['contribution_idx'][i] for i in valid_contribution_idx]\n",
    "        depth = [depth_id+1 for depth_id in cache_item['depth']] # increase all depth ids of previous contribution for one\n",
    "        \n",
    "        tasks.extend({\n",
    "            'source_idx': source_idx,\n",
    "            'target_module': new_target_module,\n",
    "            'target_vectors':new_target_vectors,\n",
    "            'target_contribution_factor': new_target_contribution_factor,\n",
    "            'target_layer': new_target_layer,\n",
    "            'target_sample_id': [cache_item['target_sample_id']] * num_tasks,\n",
    "            'target_token_pos':new_target_token_pos,\n",
    "            'depth': [depth] * num_tasks\n",
    "        })\n",
    "    tasks = merge_tasks(tasks) # create depth list with all depth of querying tokens\n",
    "    curr_tasks, tasks = get_next_curr_tasks(tasks)\n",
    "    \n",
    "\n",
    "    return curr_tasks, tasks\n",
    "\n",
    "def merge_tasks(tasks):\n",
    "    tasks.sort(by='target_sample_id')\n",
    "    tasks.sort(by='target_token_pos')\n",
    "    tasks.sort(by='source_idx', descending=True)\n",
    "\n",
    "    merged_tasks = DataDict()\n",
    "\n",
    "    prev_task = tasks[0]\n",
    "    contribution_factor_sum = torch.tensor(0, dtype=prev_task['target_contribution_factor'].dtype)\n",
    "    acc_depth_list = []\n",
    "\n",
    "    for task in tasks.to_list():\n",
    "        if task['target_sample_id'] == prev_task['target_sample_id'] \\\n",
    "            and task['source_idx'] == prev_task['source_idx'] \\\n",
    "            and task['target_token_pos'] == prev_task['target_token_pos']:\n",
    "            \n",
    "            contribution_factor_sum += task['target_contribution_factor']\n",
    "            acc_depth_list += task['depth']\n",
    "            continue\n",
    "        \n",
    "        merged_tasks.append(\n",
    "            prev_task | {'target_contribution_factor': contribution_factor_sum, 'depth':sorted(acc_depth_list)}\n",
    "        )\n",
    "        prev_task = task\n",
    "        contribution_factor_sum = task['target_contribution_factor']\n",
    "\n",
    "    merged_tasks.append(\n",
    "        prev_task | {'target_contribution_factor': contribution_factor_sum, 'depth':sorted(acc_depth_list)}\n",
    "    )      \n",
    "\n",
    "    return merged_tasks\n",
    "\n",
    "def get_next_curr_tasks(tasks):\n",
    "    tasks.sort(by='target_module', descending=True)\n",
    "    next_module = tasks[0]['target_module']\n",
    "    end_idx = next((i for i, module in enumerate(tasks['target_module']) if module != next_module), None)\n",
    "\n",
    "    if not end_idx:\n",
    "        return tasks, DataDict()\n",
    "    \n",
    "    curr_tasks = tasks[:end_idx]\n",
    "    tasks = tasks[end_idx:]\n",
    "    \n",
    "    return curr_tasks, tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacslab_lmjantsch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
