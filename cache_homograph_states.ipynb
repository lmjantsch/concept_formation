{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe26c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import json\n",
    "import sys\n",
    "import gc\n",
    "from typing import Dict, List\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, PretrainedConfig\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "from data.utils import get_entity_idx, get_prompts\n",
    "from mi_toolbox.utils.data_types import DataDict\n",
    "from mi_toolbox.utils.collate import TokenizeCollator\n",
    "from mi_toolbox.transformer_caching import caching_wrapper, decompose_attention_to_neuron, decompose_glu_to_neuron\n",
    "\n",
    "\n",
    "model_ids = [\"Qwen/Qwen3-4B\"]#, \"Qwen/Qwen3-8B\", \"Qwen/Qwen3-14B\"]  # \"Qwen/Qwen3-0.6B\",\"Qwen/Qwen3-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ids[0])\n",
    "\n",
    "data_path = os.path.join(project_root, 'data/homograph_data/homograph_small.json')\n",
    "with open(data_path) as f:\n",
    "    data = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f30e28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4c7a6d8e9b49859d2d3beb72116a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/3: 13.45 seconds\n",
      "Batch 2/3: 18.00 seconds\n",
      "Batch 3/3: 12.28 seconds\n",
      "Memory allocated: 0.04 GB\n",
      "Memory reserved: 0.05 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83dbb26181564942b0bafe4a28e0e03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/3: 17.82 seconds\n",
      "Batch 2/3: 12.57 seconds\n",
      "Batch 3/3: 9.21 seconds\n",
      "Memory allocated: 0.06 GB\n",
      "Memory reserved: 0.08 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa727c93cf14889b325e23d82f84f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/3: 12.79 seconds\n",
      "Batch 2/3: 17.98 seconds\n",
      "Batch 3/3: 4.24 seconds\n",
      "Memory allocated: 0.06 GB\n",
      "Memory reserved: 0.07 GB\n"
     ]
    }
   ],
   "source": [
    "for sample_id in range(5):\n",
    "    sample_data = [data[sample_id]]\n",
    "\n",
    "    prompts = get_prompts(sample_data, context_type='minimal_context')\n",
    "    ent_idx = get_entity_idx(tokenizer, prompts)\n",
    "\n",
    "    extract_collate_fn = TokenizeCollator(tokenizer, collate_fn={\n",
    "        'ent_idx': lambda key, value: {'batch_ent_pos_idx': (list(range(len(value))), value)}\n",
    "    })\n",
    "\n",
    "    batch_size = 4\n",
    "    extract_dd = DataDict.from_dict({'prompts': prompts, \"ent_idx\": ent_idx})\n",
    "    extract_dl = DataLoader(extract_dd, batch_size=batch_size, shuffle= False, collate_fn=extract_collate_fn)\n",
    "\n",
    "\n",
    "    layer_target = slice(0, 10)\n",
    "    def caching_function(llm: LanguageModel, config: PretrainedConfig, batch: Dict[str, List]) -> Dict:\n",
    "        try:\n",
    "            batch_cache = {}\n",
    "            batch_ent_pos_idx = batch['batch_ent_pos_idx']\n",
    "\n",
    "            batch_cache['attention_mask'] = batch['attention_mask']\n",
    "            batch_cache['ent_pos_idx'] = batch_ent_pos_idx[1]\n",
    "\n",
    "            with llm.trace(batch) as tracer:         \n",
    "                emb = llm.model.embed_tokens.output\n",
    "                batch_cache['emb'] = emb[batch_ent_pos_idx].cpu().save()\n",
    "                batch_cache['full_emb'] = emb.cpu().save()\n",
    "                \n",
    "                for i, layer in enumerate(llm.model.layers[layer_target]):\n",
    "                    attn_norm_var = torch.var(layer.input, dim=-1)\n",
    "                    \n",
    "                    # decompose attention out\n",
    "                    v_proj = layer.self_attn.v_proj.output\n",
    "                    _, attn_weight = layer.self_attn.output\n",
    "                    o_proj_WT = layer.self_attn.o_proj.weight.T\n",
    "                    d_attn = decompose_attention_to_neuron(\n",
    "                        attn_weight, \n",
    "                        v_proj, \n",
    "                        o_proj_WT,\n",
    "                        config.num_attention_heads,\n",
    "                        config.num_key_value_heads,\n",
    "                        config.head_dim\n",
    "                    ) \n",
    "                    \n",
    "                    # extract mid residual state\n",
    "                    mid = layer.post_attention_layernorm.input[batch_ent_pos_idx]\n",
    "                    mlp_norm_var = torch.var(layer.post_attention_layernorm.input, dim=-1)\n",
    "\n",
    "                    # decomposed mlp out    \n",
    "                    up_proj = layer.mlp.up_proj.output\n",
    "                    act_prod = layer.mlp.down_proj.input\n",
    "                    down_proj_WT = layer.mlp.down_proj.weight.T\n",
    "                    d_mlp = decompose_glu_to_neuron(act_prod=act_prod, down_proj_WT=down_proj_WT)\n",
    "\n",
    "                    # extract post residual state\n",
    "                    post = layer.output[batch_ent_pos_idx]\n",
    "                    \n",
    "                    # save cache\n",
    "                    batch_cache[f'{i}.d_attn'] = d_attn.cpu().save()\n",
    "                    batch_cache[f'{i}.v_proj'] = v_proj.cpu().save()\n",
    "                    batch_cache[f'{i}.attn_norm_var'] = attn_norm_var.cpu().save()\n",
    "                    batch_cache[f'{i}.mid'] = mid.cpu().save()\n",
    "                    batch_cache[f'{i}.d_mlp'] = d_mlp.cpu().save()\n",
    "                    batch_cache[f'{i}.up_proj'] = up_proj.cpu().save()\n",
    "                    batch_cache[f'{i}.mlp_norm_var'] = mlp_norm_var.cpu().save()\n",
    "                    batch_cache[f'{i}.post'] = post.cpu().save()\n",
    "        finally:\n",
    "            del tracer\n",
    "        return batch_cache\n",
    "\n",
    "    big_cache = caching_wrapper(model_ids, extract_dl, caching_function)\n",
    "\n",
    "\n",
    "    output_dir = f'/raid/dacslab/CONCEPT_FORMATION/homograph_small/Qwen_Qwen3-4B/{sample_id}/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model_cache = big_cache['Qwen/Qwen3-4B']\n",
    "\n",
    "    for key, default_value in model_cache.default_entry.items():\n",
    "\n",
    "        if isinstance(default_value, torch.Tensor):\n",
    "            output_path = os.path.join(output_dir, f\"{key}.safetensors\")\n",
    "            torch.save(model_cache[key], output_path)\n",
    "        else:\n",
    "            output_path = os.path.join(output_dir, f\"{key}.json\")\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(model_cache[key], f) \n",
    "\n",
    "    del big_cache\n",
    "    gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacslab_lmjantsch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
