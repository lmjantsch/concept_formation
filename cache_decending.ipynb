{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f9f1f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_entity_indices' from 'data.utils' (/home/dacslab/lasse_jantsch/concept_formation/data/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnnsight\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LanguageModel\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_entity_indices, get_prompts, get_patching_prompt\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'get_entity_indices' from 'data.utils' (/home/dacslab/lasse_jantsch/concept_formation/data/utils.py)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "tool_root = '/home/dacslab/lassejantsch/master_thesis/mi_toolbox/'\n",
    "if tool_root not in sys.path:\n",
    "    sys.path.append(tool_root)\n",
    "\n",
    "import gc\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from nnsight import LanguageModel\n",
    "import json\n",
    "import time\n",
    "from typing import *\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from data.utils import get_entity_indices, get_prompts,\n",
    "\n",
    "# from tools.mixin_measures import get_dot_prod_contribution\n",
    "# from tools.mixin_measures.decomposition import decompose_attention_to_neuron, decompose_glu_mlp\n",
    "# from tools.mixin_measures.contribution import get_top_x_contribution_values\n",
    "# from tools.mixin_measures.utils import distributed\n",
    "# from tools.concept_detection import get_mass_mean_vectors\n",
    "# from lib.custom_types import DataDict\n",
    "# from lib.utils import max_pad_sequence\n",
    "\n",
    "data_path = os.path.join(project_root, 'data/homograph_data/homograph_small.json')\n",
    "with open(data_path) as f:\n",
    "    data = json.load(f)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d12abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ffba6f",
   "metadata": {},
   "source": [
    "## Extract residual contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9571b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\"Qwen/Qwen3-4B\"]#, \"Qwen/Qwen3-8B\", \"Qwen/Qwen3-14B\"]  # \"Qwen/Qwen3-0.6B\",\"Qwen/Qwen3-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ids[0])\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa2867f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = get_prompts(data, context_type='minimal_context')\n",
    "ent_pos_idx = get_entity_indices(tokenizer, prompts)\n",
    "\n",
    "def extract_collate_fn(batch):\n",
    "    prompts, entity_pos_idx = zip(*batch)\n",
    "    inputs = tokenizer(\n",
    "        prompts, \n",
    "        return_tensors='pt', \n",
    "        padding=True,\n",
    "    )\n",
    "    batch_ent_pos_idx = (list(range(len(entity_pos_idx))), entity_pos_idx)\n",
    "    return inputs | {'batch_ent_pos_idx': batch_ent_pos_idx}\n",
    "\n",
    "extract_dl = DataLoader(list(zip(prompts, ent_pos_idx)), batch_size=batch_size, shuffle= False, collate_fn=extract_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc53910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2023235b938542ecac81721bb1ca398f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/1: 22.99 seconds\n",
      "Memory allocated: 0.03 GB\n",
      "Memory reserved: 6.22 GB\n"
     ]
    }
   ],
   "source": [
    "big_cache = {}\n",
    "layer_slice = slice(0,10)\n",
    "\n",
    "for model_id in model_ids:\n",
    "    try:\n",
    "        llm = LanguageModel(\n",
    "            model_id,\n",
    "            trust_remote_code=True,\n",
    "            device_map='auto',\n",
    "            dtype=torch.bfloat16,\n",
    "            attn_implementation = 'eager',\n",
    "            dispatch=True\n",
    "        )\n",
    "        head_dim, num_attn_heads, num_k_v_heads = llm.config.head_dim, llm.config.num_attention_heads, llm.config.num_key_value_heads\n",
    "        model_cache = DataDict() # TODO: create a special tensor cache dictionary that also holds config etc.\n",
    "        num_batches = len(extract_dl)\n",
    "        \n",
    "        for batch_id, batch in enumerate(extract_dl):\n",
    "            batch_ent_pos_idx = batch['batch_ent_pos_idx']\n",
    "            batch_cache = {}\n",
    "            batch_cache['attention_mask'] = batch['attention_mask']\n",
    "            batch_cache['ent_pos_idx'] = batch_ent_pos_idx[1]\n",
    "            start = time.process_time()\n",
    "            try:\n",
    "                with torch.no_grad(), llm.trace(batch) as tracer:\n",
    "                    \n",
    "                    emb = llm.model.embed_tokens.output\n",
    "                    batch_cache['emb'] = emb[batch_ent_pos_idx].cpu().save()\n",
    "                    batch_cache['full_emb'] = emb.cpu().save()\n",
    "                    \n",
    "                    for i, layer in enumerate(llm.model.layers[layer_slice]):\n",
    "                        attn_norm_var = torch.var(layer.input, dim=-1)\n",
    "                        \n",
    "                        # decompose attention out\n",
    "                        v_proj = layer.self_attn.v_proj.output\n",
    "                        _, attn_weight = layer.self_attn.output\n",
    "                        o_proj_WT = layer.self_attn.o_proj.weight.T\n",
    "                        d_attn = decompose_attention_to_neuron(\n",
    "                            attn_weight, \n",
    "                            v_proj, \n",
    "                            o_proj_WT,\n",
    "                            num_attn_heads,\n",
    "                            num_k_v_heads,\n",
    "                            head_dim\n",
    "                        ) \n",
    "                        \n",
    "                        # extract mid residual state\n",
    "                        mid = layer.post_attention_layernorm.input[batch_ent_pos_idx]\n",
    "                        mlp_norm_var = torch.var(layer.post_attention_layernorm.input, dim=-1)\n",
    "\n",
    "                        # decomposed mlp out    \n",
    "                        up_proj = layer.mlp.up_proj.output\n",
    "                        z = layer.mlp.down_proj.input\n",
    "                        down_proj_WT = layer.mlp.down_proj.weight.T\n",
    "                        d_mlp = decompose_glu_mlp(z=z, down_proj_WT=down_proj_WT)\n",
    "\n",
    "                        # extract post residual state\n",
    "                        post = layer.output[batch_ent_pos_idx]\n",
    "                        \n",
    "                        batch_cache[f'{i}.d_attn'] = d_attn.cpu().save()\n",
    "                        batch_cache[f'{i}.v_proj'] = v_proj.cpu().save()\n",
    "                        batch_cache[f'{i}.attn_norm_var'] = attn_norm_var.cpu().save()\n",
    "                        batch_cache[f'{i}.mid'] = mid.cpu().save()\n",
    "                        batch_cache[f'{i}.d_mlp'] = d_mlp.cpu().save()\n",
    "                        batch_cache[f'{i}.up_proj'] = up_proj.cpu().save()\n",
    "                        batch_cache[f'{i}.mlp_norm_var'] = mlp_norm_var.cpu().save()\n",
    "                        batch_cache[f'{i}.post'] = post.cpu().save()\n",
    "                    \n",
    "                    model_cache.extend(batch_cache)\n",
    "            finally:\n",
    "                del tracer\n",
    "                del batch_cache\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                end = time.process_time()\n",
    "                print(f\"Batch {batch_id + 1}/{num_batches}: {(end - start):.2f} seconds\")\n",
    "                \n",
    "        for key, value in model_cache.items():\n",
    "            if isinstance(value[0], torch.Tensor):\n",
    "                model_cache.replace(key, max_pad_sequence(value))\n",
    "        big_cache[model_id] = model_cache\n",
    "    finally:\n",
    "        llm.cpu()\n",
    "        del llm\n",
    "        del model_cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"Memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ef63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_vectors(big_cache, num_homographs, num_examples):\n",
    "    concept_vectors = {}\n",
    "    no_layers = 10\n",
    "\n",
    "    for model in big_cache:\n",
    "        model_cache = big_cache[model]\n",
    "        concept_vectors[model] = {}\n",
    "        \n",
    "        emb_pairs = model_cache[f'emb'].view(num_homographs, 2, num_examples, -1)\n",
    "        concept_vectors[model]['emb'] = get_mass_mean_vectors(emb_pairs)\n",
    "\n",
    "        for layer in range(no_layers):\n",
    "            mid_pairs = model_cache[f'{layer}.mid'].view(num_homographs, 2, num_examples, -1)\n",
    "            post_pairs = model_cache[f'{layer}.post'].view(num_homographs, 2, num_examples, -1)\n",
    "            \n",
    "            concept_vectors[model][f'{layer}.mid'] = get_mass_mean_vectors(mid_pairs)\n",
    "            concept_vectors[model][f'{layer}.post'] = get_mass_mean_vectors(post_pairs)\n",
    "    \n",
    "    return concept_vectors\n",
    "\n",
    "concept_vectors = get_concept_vectors(big_cache, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43133429",
   "metadata": {},
   "source": [
    "### Trace Contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d908ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_contribution_parts(cache: dict, stop_layer: int, batch_token_pos_idx: List[Tuple]):\n",
    "    \"\"\"Returns all contributing parts until the stop layer\n",
    "\n",
    "    Args:\n",
    "        cache (dict): batch cache dictionary\n",
    "        stop_layer (int): index of last included layer (zero indexing)\n",
    "        batch_token_position_idx (List[Tuple]): Batch and token position indexes (sample_idx, token_pos_idx)\n",
    "\n",
    "    Returns:\n",
    "        Tensor: all contributing parts until stop layer (batch, flat_pats)\n",
    "    \"\"\"\n",
    "    \n",
    "    parts = []\n",
    "    parts.append(cache['full_emb'][batch_token_pos_idx][:,None])\n",
    "    \n",
    "    for layer in range(stop_layer):\n",
    "        parts.append(cache[f'{layer}.d_attn'][batch_token_pos_idx].flatten(1, -2))\n",
    "        parts.append(cache[f\"{layer}.d_mlp\"][batch_token_pos_idx])\n",
    "    parts = torch.concat(parts, dim=1).transpose(0,1)\n",
    "    \n",
    "    return parts\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_target_vectors_and_postions(llm, cache_item, model_cache):\n",
    "    contribution_modules = cache_item['contribution_modules']\n",
    "    contribution_values = cache_item['contribution_values']\n",
    "    sample_id = cache_item['target_sample_id']\n",
    "    source_token_pos = cache_item['target_token_pos']\n",
    "    eps = 1e-7\n",
    "\n",
    "    num_attn_heads = llm.config.num_attention_heads\n",
    "    num_k_v_heads = llm.config.num_key_value_heads\n",
    "    num_head_groups = num_attn_heads // num_k_v_heads\n",
    "    head_dim = llm.config.head_dim\n",
    "    hidden_dim = llm.config.hidden_size\n",
    "\n",
    "    new_target_vectors = []\n",
    "    new_target_token_pos = []\n",
    "    new_target_module = []\n",
    "    new_target_layer = []\n",
    "    new_target_contribution_factor = []\n",
    "    valid_contribution_idx = []\n",
    "\n",
    "    for i,( module_desc, cont_value) in enumerate(zip(contribution_modules, contribution_values)):\n",
    "        if module_desc == 'emb':continue\n",
    "        \n",
    "        layer_id, module_type, target_token_pos, head_pos = module_desc.split('.')\n",
    "        layer_id, head_pos, target_token_pos = int(layer_id), int(head_pos), int(target_token_pos)\n",
    "        \n",
    "        if target_token_pos == -1:\n",
    "            target_token_pos = source_token_pos\n",
    "        \n",
    "        new_target_token_pos.append(target_token_pos)\n",
    "        new_target_module.append(f\"{layer_id}.{module_type}\")\n",
    "        new_target_layer.append(layer_id)\n",
    "        valid_contribution_idx.append(i)\n",
    "        \n",
    "        if module_type == 'attn':\n",
    "            v_proj_W = llm.layers[layer_id].self_attn.v_proj.weight.data.view(num_k_v_heads, head_dim, hidden_dim)[head_pos//head_dim//num_head_groups, head_pos % head_dim]\n",
    "            norm = llm.layers[layer_id].input_layernorm.weight.data\n",
    "            \n",
    "            v_proj = model_cache[f\"{layer_id}.v_proj\"][sample_id, target_token_pos].view(num_k_v_heads, head_dim)[head_pos//head_dim//num_head_groups, head_pos % head_dim]\n",
    "            norm_var = model_cache[f\"{layer_id}.attn_norm_var\"][sample_id, source_token_pos] \n",
    "            \n",
    "            target_vector = v_proj_W * norm / norm_var.cuda()\n",
    "            target_contribution_factor = (cont_value / v_proj.cuda())\n",
    "            new_target_contribution_factor.append(target_contribution_factor.cpu())\n",
    "            \n",
    "        elif module_type == 'mlp':\n",
    "            up_proj_W = llm.layers[layer_id].mlp.up_proj.weight.data[head_pos]\n",
    "            norm = llm.layers[layer_id].post_attention_layernorm.weight.data\n",
    "        \n",
    "            up_proj =  model_cache[f\"{layer_id}.up_proj\"][sample_id, target_token_pos][head_pos]\n",
    "            norm_var = model_cache[f\"{layer_id}.mlp_norm_var\"][sample_id, source_token_pos]\n",
    "            \n",
    "            target_vector = up_proj_W * norm / norm_var.cuda()\n",
    "            target_contribution_factor = (cont_value / up_proj.cuda())\n",
    "            new_target_contribution_factor.append(target_contribution_factor.cpu())\n",
    "        \n",
    "        new_target_vectors.append(target_vector.cpu())\n",
    "        \n",
    "    new_target_vectors = torch.stack(new_target_vectors)\n",
    "\n",
    "    return new_target_vectors, new_target_token_pos, new_target_layer, new_target_module, new_target_contribution_factor, valid_contribution_idx\n",
    "\n",
    "def apply_target_mask(contributions, first_target_module_group_idx):\n",
    "    \n",
    "    batch_size, contribution_size = contributions.shape\n",
    "    \n",
    "    contribution_mask = torch.arange(0, contribution_size)[None, :].repeat(batch_size, 1)\n",
    "    contribution_mask = contribution_mask < torch.tensor(first_target_module_group_idx)[:, None]\n",
    "    \n",
    "    contributions[~contribution_mask] = 0\n",
    "    \n",
    "    return contributions\n",
    "\n",
    "def get_module_list(llm, target_layer, num_tokens):\n",
    "    num_attn_heads, attn_head_dim, num_mlp_heads = llm.config.num_attention_heads, llm.config.head_dim, llm.config.intermediate_size\n",
    "    module_list = ['emb']\n",
    "    for layer_id in range(target_layer + 1):\n",
    "        module_list.extend([f\"{layer_id}.attn.{token_pos}.{head_pos}\" for token_pos in range(num_tokens) for head_pos in range(num_attn_heads * attn_head_dim)])\n",
    "        module_list.extend([f\"{layer_id}.mlp.-1.{head_pos}\" for head_pos in range(num_mlp_heads)])\n",
    "    \n",
    "    return module_list\n",
    "\n",
    "def get_first_module_group_id_lookup(module_list):\n",
    "    first_module_group_id_lookup = {}\n",
    "    prev_layer, prev_module_type = 0, ''\n",
    "    for i, module in enumerate(module_list):\n",
    "        if module == 'emb': continue\n",
    "        \n",
    "        curr_layer, curr_module_type, _, _ = module.split('.')\n",
    "        if not first_module_group_id_lookup or \\\n",
    "            curr_layer != prev_layer or curr_module_type != prev_module_type:\n",
    "            first_module_group_id_lookup[f\"{curr_layer}.{curr_module_type}\"] = i\n",
    "        \n",
    "        prev_layer, prev_module_type = curr_layer, curr_module_type\n",
    "        \n",
    "    return first_module_group_id_lookup\n",
    "\n",
    "def get_initial_tasks(concept_vectors, target_module, target_layer, num_examples):\n",
    "    tasks = DataDict(length=num_examples)\n",
    "        \n",
    "    target_vectors = concept_vectors[model_id][target_module]\n",
    "    target_vectors = torch.flatten(target_vectors[:, None].repeat(1, 8, 1) * torch.tensor([[1]] * 4 + [[-1]] * 4), 0, 1)\n",
    "    tasks.attach('target_vectors', target_vectors, force= True)\n",
    "    \n",
    "    tasks.attach('target_sample_id', range(num_examples), force= True)\n",
    "    tasks.attach('target_contribution_factor', torch.tensor([1] * num_examples), force= True)\n",
    "    tasks.attach('target_token_pos', model_cache['ent_pos_idx'], force= True)\n",
    "    tasks.attach('target_layer', [target_layer] * num_examples, force= True)\n",
    "    tasks.attach('target_module', [target_module] * num_examples, force= True)\n",
    "    tasks.attach('depth', [[0]] * num_examples, force= True)\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "def merge_tasks(tasks):\n",
    "    tasks.sort(by='target_sample_id')\n",
    "    tasks.sort(by='target_token_pos')\n",
    "    tasks.sort(by='source_idx', descending=True)\n",
    "\n",
    "    merged_tasks = DataDict()\n",
    "\n",
    "    prev_task = tasks[0]\n",
    "    contribution_factor_sum = torch.tensor(0, dtype=prev_task['target_contribution_factor'].dtype)\n",
    "    acc_depth_list = []\n",
    "\n",
    "    for task in tasks.to_list():\n",
    "        if task['target_sample_id'] == prev_task['target_sample_id'] \\\n",
    "            and task['source_idx'] == prev_task['source_idx'] \\\n",
    "            and task['target_token_pos'] == prev_task['target_token_pos']:\n",
    "            \n",
    "            contribution_factor_sum += task['target_contribution_factor']\n",
    "            acc_depth_list += task['depth']\n",
    "            continue\n",
    "        \n",
    "        merged_tasks.append(\n",
    "            prev_task | {'target_contribution_factor': contribution_factor_sum, 'depth':sorted(acc_depth_list)}\n",
    "        )\n",
    "        prev_task = task\n",
    "        contribution_factor_sum = task['target_contribution_factor']\n",
    "\n",
    "    merged_tasks.append(\n",
    "        prev_task | {'target_contribution_factor': contribution_factor_sum, 'depth':sorted(acc_depth_list)}\n",
    "    )      \n",
    "\n",
    "    return merged_tasks\n",
    "\n",
    "def get_next_curr_tasks(tasks):\n",
    "    tasks.sort(by='target_module', descending=True)\n",
    "    next_module = tasks[0]['target_module']\n",
    "    end_idx = next((i for i, module in enumerate(tasks['target_module']) if module != next_module), None)\n",
    "\n",
    "    if not end_idx:\n",
    "        return tasks, DataDict()\n",
    "    \n",
    "    curr_tasks = tasks[:end_idx]\n",
    "    tasks = tasks[end_idx:]\n",
    "    \n",
    "    return curr_tasks, tasks\n",
    "\n",
    "def get_contribution_pars_cache(model_cache, stop_layer):\n",
    "    stop_layer = 9\n",
    "    parts = []\n",
    "    parts.append(model_cache['full_emb'][:,:,None])\n",
    "\n",
    "    for layer in range(stop_layer + 1):\n",
    "        parts.append(model_cache[f'{layer}.d_attn'].flatten(2, -2))\n",
    "        parts.append(model_cache[f\"{layer}.d_mlp\"])\n",
    "    parts = torch.concat(parts, dim=2)\n",
    "\n",
    "    parts -= parts.mean(dim=-1, keepdim=True)\n",
    "    \n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7272aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch_dict = {}\n",
    "    \n",
    "    for key, exampel_val in batch[0].items():\n",
    "        if isinstance(exampel_val, torch.Tensor):\n",
    "            batch_dict[key] = max_pad_sequence([sample[key] for sample in batch])\n",
    "            continue\n",
    "        batch_dict[key] = [sample[key] for sample in batch]\n",
    "    \n",
    "    return batch_dict | {'length': len(batch_dict[key])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd5baa98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027d3d315c354ce98c2c8ec721d4ed12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0/20:\n",
      "Parts time: (1.82s)\n",
      "\tBatch 1/1: 2.88 seconds\n",
      "\n",
      "Step 1/20:\n",
      "Parts time: (1.30s)\n",
      "\tBatch 1/7: 3.14 seconds\n",
      "Parts time: (1.74s)\n",
      "\tBatch 2/7: 3.64 seconds\n",
      "Parts time: (1.69s)\n",
      "\tBatch 3/7: 3.53 seconds\n",
      "Parts time: (1.74s)\n",
      "\tBatch 4/7: 3.63 seconds\n",
      "Parts time: (1.69s)\n",
      "\tBatch 5/7: 3.53 seconds\n",
      "Parts time: (1.74s)\n",
      "\tBatch 6/7: 3.65 seconds\n",
      "Parts time: (1.68s)\n",
      "\tBatch 7/7: 3.41 seconds\n",
      "\n",
      "Step 2/20:\n",
      "Parts time: (1.19s)\n",
      "\tBatch 1/1: 1.91 seconds\n",
      "\n",
      "Step 3/20:\n",
      "Parts time: (1.07s)\n",
      "\tBatch 1/9: 2.72 seconds\n",
      "Parts time: (1.56s)\n",
      "\tBatch 2/9: 3.26 seconds\n",
      "Parts time: (1.51s)\n",
      "\tBatch 3/9: 3.17 seconds\n",
      "Parts time: (1.57s)\n",
      "\tBatch 4/9: 3.26 seconds\n",
      "Parts time: (1.51s)\n",
      "\tBatch 5/9: 3.19 seconds\n",
      "Parts time: (1.56s)\n",
      "\tBatch 6/9: 3.24 seconds\n",
      "Parts time: (1.50s)\n",
      "\tBatch 7/9: 3.12 seconds\n",
      "Parts time: (1.56s)\n",
      "\tBatch 8/9: 3.24 seconds\n",
      "Parts time: (1.18s)\n",
      "\tBatch 9/9: 2.02 seconds\n",
      "\n",
      "Step 4/20:\n",
      "Parts time: (0.47s)\n",
      "\tBatch 1/1: 0.58 seconds\n",
      "\n",
      "Step 5/20:\n",
      "Parts time: (0.73s)\n",
      "\tBatch 1/13: 2.20 seconds\n",
      "Parts time: (1.38s)\n",
      "\tBatch 2/13: 2.88 seconds\n",
      "Parts time: (1.34s)\n",
      "\tBatch 3/13: 2.79 seconds\n",
      "Parts time: (1.36s)\n",
      "\tBatch 4/13: 2.83 seconds\n",
      "Parts time: (1.35s)\n",
      "\tBatch 5/13: 2.80 seconds\n",
      "Parts time: (1.37s)\n",
      "\tBatch 6/13: 2.84 seconds\n",
      "Parts time: (1.34s)\n",
      "\tBatch 7/13: 2.80 seconds\n",
      "Parts time: (1.37s)\n",
      "\tBatch 8/13: 2.84 seconds\n",
      "Parts time: (1.34s)\n",
      "\tBatch 9/13: 2.82 seconds\n",
      "Parts time: (1.36s)\n",
      "\tBatch 10/13: 2.85 seconds\n",
      "Parts time: (1.36s)\n",
      "\tBatch 11/13: 2.97 seconds\n",
      "Parts time: (1.39s)\n",
      "\tBatch 12/13: 3.00 seconds\n",
      "Parts time: (1.23s)\n",
      "\tBatch 13/13: 2.40 seconds\n",
      "\n",
      "Step 6/20:\n",
      "Parts time: (0.60s)\n",
      "\tBatch 1/1: 0.70 seconds\n",
      "\n",
      "Step 7/20:\n",
      "Parts time: (0.60s)\n",
      "\tBatch 1/7: 1.92 seconds\n",
      "Parts time: (1.17s)\n",
      "\tBatch 2/7: 2.47 seconds\n",
      "Parts time: (1.19s)\n",
      "\tBatch 3/7: 2.49 seconds\n",
      "Parts time: (1.17s)\n",
      "\tBatch 4/7: 2.47 seconds\n",
      "Parts time: (1.18s)\n",
      "\tBatch 5/7: 2.47 seconds\n",
      "Parts time: (1.17s)\n",
      "\tBatch 6/7: 2.48 seconds\n",
      "Parts time: (1.18s)\n",
      "\tBatch 7/7: 2.47 seconds\n",
      "\n",
      "Step 8/20:\n",
      "Parts time: (0.77s)\n",
      "\tBatch 1/1: 1.10 seconds\n",
      "\n",
      "Step 9/20:\n",
      "Parts time: (0.62s)\n",
      "\tBatch 1/6: 1.71 seconds\n",
      "Parts time: (0.99s)\n",
      "\tBatch 2/6: 2.08 seconds\n",
      "Parts time: (0.98s)\n",
      "\tBatch 3/6: 2.09 seconds\n",
      "Parts time: (0.99s)\n",
      "\tBatch 4/6: 2.09 seconds\n",
      "Parts time: (0.98s)\n",
      "\tBatch 5/6: 2.10 seconds\n",
      "Parts time: (0.74s)\n",
      "\tBatch 6/6: 1.22 seconds\n",
      "\n",
      "Step 10/20:\n",
      "Parts time: (0.44s)\n",
      "\tBatch 1/1: 0.93 seconds\n",
      "\n",
      "Step 11/20:\n",
      "Parts time: (0.61s)\n",
      "\tBatch 1/3: 1.49 seconds\n",
      "Parts time: (0.81s)\n",
      "\tBatch 2/3: 1.68 seconds\n",
      "Parts time: (0.70s)\n",
      "\tBatch 3/3: 1.37 seconds\n",
      "\n",
      "Step 12/20:\n",
      "Parts time: (0.42s)\n",
      "\tBatch 1/1: 0.65 seconds\n",
      "\n",
      "Step 13/20:\n",
      "Parts time: (0.38s)\n",
      "\tBatch 1/2: 1.04 seconds\n",
      "Parts time: (0.59s)\n",
      "\tBatch 2/2: 1.21 seconds\n",
      "\n",
      "Step 14/20:\n",
      "Parts time: (0.49s)\n",
      "\tBatch 1/16: 0.94 seconds\n",
      "Parts time: (0.41s)\n",
      "\tBatch 2/16: 0.85 seconds\n",
      "Parts time: (0.40s)\n",
      "\tBatch 3/16: 0.85 seconds\n",
      "Parts time: (0.41s)\n",
      "\tBatch 4/16: 0.85 seconds\n",
      "Parts time: (0.41s)\n",
      "\tBatch 5/16: 0.85 seconds\n",
      "Parts time: (0.41s)\n",
      "\tBatch 6/16: 0.85 seconds\n",
      "Parts time: (0.41s)\n",
      "\tBatch 7/16: 0.86 seconds\n",
      "Parts time: (0.41s)\n",
      "\tBatch 8/16: 0.85 seconds\n",
      "Parts time: (0.41s)\n",
      "\tBatch 9/16: 0.86 seconds\n",
      "Parts time: (0.41s)\n",
      "\tBatch 10/16: 0.86 seconds\n",
      "Parts time: (0.41s)\n",
      "\tBatch 11/16: 0.86 seconds\n",
      "Parts time: (0.41s)\n",
      "\tBatch 12/16: 0.86 seconds\n",
      "Parts time: (0.41s)\n",
      "\tBatch 13/16: 0.86 seconds\n",
      "Parts time: (0.41s)\n",
      "\tBatch 14/16: 0.86 seconds\n",
      "Parts time: (0.41s)\n",
      "\tBatch 15/16: 0.87 seconds\n",
      "Parts time: (0.25s)\n",
      "\tBatch 16/16: 0.32 seconds\n",
      "\n",
      "Step 15/20:\n",
      "Parts time: (0.21s)\n",
      "\tBatch 1/4: 0.66 seconds\n",
      "Parts time: (0.41s)\n",
      "\tBatch 2/4: 0.86 seconds\n",
      "Parts time: (0.41s)\n",
      "\tBatch 3/4: 0.86 seconds\n",
      "Parts time: (0.38s)\n",
      "\tBatch 4/4: 0.75 seconds\n",
      "\n",
      "Step 16/20:\n",
      "Parts time: (0.28s)\n",
      "\tBatch 1/6: 0.51 seconds\n",
      "Parts time: (0.21s)\n",
      "\tBatch 2/6: 0.44 seconds\n",
      "Parts time: (0.21s)\n",
      "\tBatch 3/6: 0.44 seconds\n",
      "Parts time: (0.21s)\n",
      "\tBatch 4/6: 0.44 seconds\n",
      "Parts time: (0.21s)\n",
      "\tBatch 5/6: 0.44 seconds\n",
      "Parts time: (0.21s)\n",
      "\tBatch 6/6: 0.44 seconds\n",
      "\n",
      "Step 17/20:\n",
      "Parts time: (0.21s)\n",
      "\tBatch 1/2: 0.44 seconds\n",
      "Parts time: (0.16s)\n",
      "\tBatch 2/2: 0.28 seconds\n",
      "\n",
      "Step 18/20:\n",
      "Parts time: (0.06s)\n",
      "\tBatch 1/47: 0.06 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 2/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 3/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 4/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 5/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 6/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 7/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 8/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 9/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 10/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 11/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 12/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 13/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 14/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 15/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 16/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 17/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 18/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 19/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 20/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 21/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 22/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 23/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 24/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 25/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 26/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 27/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 28/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 29/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 30/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 31/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 32/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 33/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 34/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 35/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 36/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 37/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 38/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 39/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 40/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 41/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 42/47: 0.01 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 43/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 44/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 45/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 46/47: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 47/47: 0.00 seconds\n",
      "\n",
      "Step 19/20:\n",
      "Parts time: (0.00s)\n",
      "\tBatch 1/5: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 2/5: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 3/5: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 4/5: 0.00 seconds\n",
      "Parts time: (0.00s)\n",
      "\tBatch 5/5: 0.00 seconds\n",
      "Memory allocated: 22.31 GB\n",
      "Memory reserved: 28.49 GB\n"
     ]
    }
   ],
   "source": [
    "target_module = '9.post'\n",
    "target_layer = 10\n",
    "contribution_cache = []\n",
    "batch_size = 16\n",
    "max_depth = 20\n",
    "\n",
    "for model_id in big_cache:\n",
    "    try:\n",
    "        llm = AutoModel.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=True,\n",
    "            device_map='auto',\n",
    "            dtype=torch.bfloat16,\n",
    "            attn_implementation = 'eager'\n",
    "        )\n",
    "        llm.layers[target_layer:].to('meta')\n",
    "        model_cache = big_cache[model_id]\n",
    "        num_examples = model_cache.length\n",
    "        num_tokens = model_cache['0.d_attn'].size(1)\n",
    "        \n",
    "        module_list = get_module_list(llm, target_layer, num_tokens)\n",
    "        first_module_group_id_lookup = get_first_module_group_id_lookup(module_list)\n",
    "        tasks = DataDict()\n",
    "        curr_tasks = get_initial_tasks(concept_vectors, target_module, target_layer, num_examples)\n",
    "        \n",
    "        parts_cache = get_contribution_pars_cache(model_cache, target_layer)\n",
    "        \n",
    "        for depth in range(max_depth):\n",
    "            print(f\"\\nStep {depth}/{max_depth}:\")\n",
    "            contribution_cache.append(DataDict())\n",
    "            \n",
    "            dl = DataLoader(curr_tasks, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "            num_batches = len(dl)\n",
    "            \n",
    "            for batch_id, batch in enumerate(dl):\n",
    "                start = time.time()\n",
    "                curr_batch_size = batch['length']\n",
    "                batch_start_time = time.time()\n",
    "                try:\n",
    "                    target_vectors = batch['target_vectors']\n",
    "                    sample_id = batch['target_sample_id']\n",
    "                    token_pos = batch['target_token_pos']\n",
    "                    target_layer = batch['target_layer']\n",
    "                    \n",
    "                    max_target_layer = max(target_layer)\n",
    "                    \n",
    "                    if depth==0:\n",
    "                        parts = get_contribution_parts(model_cache, max_target_layer, batch_token_pos_idx=(sample_id, token_pos))\n",
    "                    else: \n",
    "                        contribution_ceiling = first_module_group_id_lookup[f\"{max_target_layer}.attn\"]\n",
    "                        parts = parts_cache[sample_id, token_pos, :contribution_ceiling].transpose(0, 1)\n",
    "                        \n",
    "                    \n",
    "                    print(f\"Parts time: ({(time.time()-start):.2f}s)\")\n",
    "                    contributions = get_dot_prod_contribution(parts = parts, whole= target_vectors).transpose(0, 1)\n",
    "                    # contributions = distributed(\n",
    "                    #     mixin_measure = get_dot_prod_contribution,\n",
    "                    #     parts = parts, whole= target_vectors, device='cuda', chunk_size=50000).transpose(0, 1)\n",
    "                    \n",
    "                    if depth > 0:\n",
    "                        first_target_module_group_idx = [first_module_group_id_lookup[module] for module in batch['target_module']]\n",
    "                        contributions = apply_target_mask(contributions, first_target_module_group_idx)\n",
    "                    \n",
    "                    scaled_contributions = contributions * batch['target_contribution_factor'][:, None]\n",
    "                    \n",
    "                    top_x_contributions = get_top_x_contribution_values(scaled_contributions, 0.9) # does this make sense anymore?\n",
    "\n",
    "                    contribution_idx = [el.nonzero(as_tuple=True)[0] for el in top_x_contributions]\n",
    "                    contribution_values = [top_x_contributions[i][idx] for i, idx in enumerate(contribution_idx)]\n",
    "                    contribution_modules = [[module_list[i] for i in idx] for idx in contribution_idx]\n",
    "                    \n",
    "                    contribution_cache[-1].extend({\n",
    "                        'contribution_idx': contribution_idx,\n",
    "                        'contribution_values':contribution_values,\n",
    "                        'contribution_modules':contribution_modules,\n",
    "                        'target_sample_id': sample_id,\n",
    "                        'target_token_pos': token_pos,\n",
    "                        'target_module': batch['target_module'],\n",
    "                        'depth': batch['depth'],\n",
    "                        })\n",
    "                finally:\n",
    "                    batch_end_time = time.time()\n",
    "                    print(f\"\\tBatch {batch_id + 1}/{num_batches}: {(batch_end_time - batch_start_time):.2f} seconds\")\n",
    "            \n",
    "            for cache_item in contribution_cache[-1].to_list():\n",
    "                if not cache_item['contribution_modules'] or torch.all(cache_item['contribution_idx'] == 0): continue\n",
    "                new_target_vectors, new_target_token_pos, new_target_layer, new_target_module, new_target_contribution_factor, valid_contribution_idx = get_target_vectors_and_postions(llm, cache_item, model_cache)\n",
    "\n",
    "                num_tasks = len(new_target_vectors)\n",
    "                source_idx = [cache_item['contribution_idx'][i] for i in valid_contribution_idx]\n",
    "                depth = [depth_id+1 for depth_id in cache_item['depth']] # increase all depth ids of previous contribution for one\n",
    "                \n",
    "                tasks.extend({\n",
    "                    'source_idx': source_idx,\n",
    "                    'target_module': new_target_module,\n",
    "                    'target_vectors':new_target_vectors,\n",
    "                    'target_contribution_factor': new_target_contribution_factor,\n",
    "                    'target_layer': new_target_layer,\n",
    "                    'target_sample_id': [cache_item['target_sample_id']] * num_tasks,\n",
    "                    'target_token_pos':new_target_token_pos,\n",
    "                    'depth': [depth] * num_tasks\n",
    "                })\n",
    "            if tasks.length == 0: break\n",
    "            tasks = merge_tasks(tasks) # create depth list with all depth of querying tokens\n",
    "            curr_tasks, tasks = get_next_curr_tasks(tasks)\n",
    "    finally:\n",
    "        del llm\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"Memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8632629a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contribution_idx</th>\n",
       "      <th>contribution_values</th>\n",
       "      <th>target_sample_id</th>\n",
       "      <th>target_token_pos</th>\n",
       "      <th>depth</th>\n",
       "      <th>layer</th>\n",
       "      <th>module_type</th>\n",
       "      <th>token_pos</th>\n",
       "      <th>head_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>113378</td>\n",
       "      <td>1.359375</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>3</td>\n",
       "      <td>mlp</td>\n",
       "      <td>4</td>\n",
       "      <td>2273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142508</td>\n",
       "      <td>0.261719</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>4</td>\n",
       "      <td>mlp</td>\n",
       "      <td>4</td>\n",
       "      <td>1195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180371</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>5</td>\n",
       "      <td>mlp</td>\n",
       "      <td>4</td>\n",
       "      <td>8850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>203391</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>6</td>\n",
       "      <td>mlp</td>\n",
       "      <td>4</td>\n",
       "      <td>1662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>207013</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>6</td>\n",
       "      <td>mlp</td>\n",
       "      <td>4</td>\n",
       "      <td>5284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   contribution_idx  contribution_values  target_sample_id  target_token_pos  \\\n",
       "0            113378             1.359375                 0                 4   \n",
       "1            142508             0.261719                 0                 4   \n",
       "2            180371             3.125000                 0                 4   \n",
       "3            203391             0.122070                 0                 4   \n",
       "4            207013             0.464844                 0                 4   \n",
       "\n",
       "  depth  layer module_type  token_pos  head_id  \n",
       "0   [0]      3         mlp          4     2273  \n",
       "1   [0]      4         mlp          4     1195  \n",
       "2   [0]      5         mlp          4     8850  \n",
       "3   [0]      6         mlp          4     1662  \n",
       "4   [0]      6         mlp          4     5284  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for cache in contribution_cache:\n",
    "    for contribution in cache.to_list():\n",
    "        if not contribution['contribution_modules']: continue\n",
    "        num_contributions = len(contribution['contribution_modules'])\n",
    "        contribution_df = pd.DataFrame(data={\n",
    "            'contribution_idx': contribution['contribution_idx'].tolist(),\n",
    "            'contribution_values':contribution['contribution_values'].tolist(),\n",
    "            'contribution_modules':contribution['contribution_modules'],\n",
    "            'target_sample_id':[contribution['target_sample_id']] * num_contributions,\n",
    "            'target_token_pos':[contribution['target_token_pos']] * num_contributions,\n",
    "            'depth':[contribution['depth']] * num_contributions,\n",
    "            })\n",
    "        df = pd.concat([df, contribution_df]).reset_index(drop=True)\n",
    "        \n",
    "df['layer'] = df['contribution_modules'].apply(lambda x: int(x.split('.')[0]) if x != 'emb' else 0)\n",
    "df['module_type'] = pd.Categorical(df['contribution_modules'].apply(lambda x: x.split('.')[1] if x != 'emb' else 'emb'), categories=['emb', 'attn', 'mlp'], ordered=True)\n",
    "df['token_pos'] = df.apply(lambda x: int(x['contribution_modules'].split('.')[2]) if x['contribution_modules'] != 'emb' and int(x['contribution_modules'].split('.')[2]) != -1  else x['target_token_pos'], axis=1)\n",
    "df['head_id'] = df['contribution_modules'].apply(lambda x: int(x.split('.')[3]) if x != 'emb' else 0)\n",
    "df = df.drop('contribution_modules', axis=1)\n",
    "\n",
    "data_path = os.path.join(project_root, 'data/contribution_cache/cache_9_post_with_norm_hom_1.csv')\n",
    "df.to_csv(data_path)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacslab_lmjantsch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
