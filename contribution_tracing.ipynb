{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4739c9d",
   "metadata": {},
   "source": [
    "## Task List\n",
    "\n",
    "* Create Graph based version\n",
    "    - Each module is a node\n",
    "    - Each contribution is an edge\n",
    "    - Update Node with for ecach incomming (and outgoing) edge\n",
    "* Change to bi-directional batching\n",
    "    - Start with last token -> load parts -> calculate contribution for each layer -> repeat for previous token until no open nodes are left.\n",
    "    - Less memory intensive (Only load cache for one token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503273e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "import json\n",
    "import sys\n",
    "import gc\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, PretrainedConfig, AutoConfig\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from nnsight import LanguageModel\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from data.utils import get_entity_idx, get_prompts\n",
    "from mi_toolbox.utils.data_types import DataDict\n",
    "from mi_toolbox.utils.collate import TensorCollator\n",
    "from mi_toolbox.transformer_caching import caching_wrapper, decompose_attention_to_neuron, decompose_glu_to_neuron, TransformerCache\n",
    "from mi_toolbox.causal_tracing import get_mass_mean_vectors\n",
    "from mi_toolbox.contribution_tracing import get_dot_prod_contribution, get_top_x_contribution_values\n",
    "\n",
    "\n",
    "model_ids = [\"Qwen/Qwen3-4B\"]#, \"Qwen/Qwen3-8B\", \"Qwen/Qwen3-14B\"]  # \"Qwen/Qwen3-0.6B\",\"Qwen/Qwen3-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2f87a",
   "metadata": {},
   "source": [
    "### Get Concept Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac8a81",
   "metadata": {},
   "source": [
    "### Get direct contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e402dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_lookup(model_config: PretrainedConfig, target_layer: int, num_tokens: int) -> List[str]:\n",
    "    num_attn_heads, attn_head_dim, num_mlp_heads = model_config.num_attention_heads, model_config.head_dim, model_config.intermediate_size\n",
    "    module_list = ['emb']\n",
    "\n",
    "    for layer_id in range(target_layer):\n",
    "        module_list.extend([f\"{layer_id}.attn.{token_pos}.{head_pos}\" for token_pos in range(num_tokens) for head_pos in range(num_attn_heads * attn_head_dim)])\n",
    "        module_list.extend([f\"{layer_id}.mlp.-1.{head_pos}\" for head_pos in range(num_mlp_heads)])\n",
    "    \n",
    "    return module_list\n",
    "\n",
    "def get_first_module_group_id_lookup(module_lookup: List[str]) -> Dict[str, int]:\n",
    "    first_module_group_id_lookup = {}\n",
    "    prev_layer, prev_module_type = 0, ''\n",
    "    for i, module in enumerate(module_lookup):\n",
    "        if module == 'emb': continue\n",
    "        \n",
    "        curr_layer, curr_module_type, _, _ = module.split('.')\n",
    "        if not first_module_group_id_lookup or \\\n",
    "            curr_layer != prev_layer or curr_module_type != prev_module_type:\n",
    "            first_module_group_id_lookup[f\"{curr_layer}.{curr_module_type}\"] = i\n",
    "        \n",
    "        prev_layer, prev_module_type = curr_layer, curr_module_type\n",
    "        \n",
    "    return first_module_group_id_lookup\n",
    "\n",
    "def get_initial_tasks(concept_vectors: torch.Tensor, target_module: str, num_examples: int, target_token_pos: List[int]) -> DataDict:\n",
    "    tasks = DataDict(length=num_examples)\n",
    "        \n",
    "    target_vectors = concept_vectors[target_module]\n",
    "    target_vectors = torch.flatten(target_vectors[:, None].repeat(1, 10, 1) * torch.tensor([[1]] * 5 + [[-1]] * 5), 0, 1)\n",
    "    tasks.attach('target_idx', [None] * num_examples)\n",
    "    tasks.attach('target_vectors', target_vectors, force= True)\n",
    "    tasks.attach('target_contribution_factor', torch.tensor([1] * num_examples), force= True)\n",
    "    tasks.attach('target_sample_ids', range(num_examples), force= True)\n",
    "    tasks.attach('target_token_pos', target_token_pos, force= True)\n",
    "    tasks.attach('depth', [[0]] * num_examples, force= True)\n",
    "\n",
    "    return tasks\n",
    "\n",
    "def mean_shift(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    return tensor - tensor.mean(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa3ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from typing import Generator, Tuple, Iterator, Union, List, Dict\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "@contextmanager\n",
    "def load_language_model(\n",
    "    model_id: Union[str, List[str]],\n",
    "    **kwargs\n",
    ") -> Generator[Tuple[PreTrainedModel, PretrainedConfig], None, None]:\n",
    "    llm = AutoModel.from_pretrained(\n",
    "        model_id,\n",
    "        **kwargs\n",
    "    )\n",
    "    try:\n",
    "        yield llm, llm.config\n",
    "    finally:\n",
    "        llm.cpu()\n",
    "        del llm\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"Memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "def batch_iterator(dl: DataLoader) -> Iterator[Dict, ]:\n",
    "    for batch_id, batch in enumerate(dl):\n",
    "        start_time = time.perf_counter()\n",
    "        try:\n",
    "            yield batch\n",
    "        finally:\n",
    "            end_time = time.perf_counter()\n",
    "            print(f\"Batch {batch_id + 1}/{len(dl)}: {(end_time - start_time):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c76a0482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contribution_ceiling_id(target_module: str, module_lookup: List[str]):\n",
    "    module_type = '.'.join(target_module.split('.')[:2])\n",
    "    first_module_id = next(idx for idx, module in enumerate(module_lookup) if module.startswith(module_type))\n",
    "    return first_module_id\n",
    "\n",
    "def get_batch_parts(batch: Dict, parts_cache: torch.Tensor, contribution_ceiling_id: int):\n",
    "    target_sample_ids = batch['target_sample_ids']\n",
    "    target_token_pos = batch['target_token_pos']\n",
    "\n",
    "    batch_parts = parts_cache[target_sample_ids, target_token_pos, :contribution_ceiling_id].transpose(0, 1)\n",
    "    return batch_parts\n",
    "\n",
    "def get_contributions(batch: Dict, batch_parts: torch.Tensor) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "    target_vectors = batch['target_vectors']\n",
    "    target_contribution_factor = batch['target_contribution_factor']\n",
    "\n",
    "    contributions = get_dot_prod_contribution(parts=batch_parts, whole=target_vectors).transpose(0, 1)\n",
    "    scaled_contributions = contributions * target_contribution_factor[:, None]\n",
    "\n",
    "    top_x_contributions = get_top_x_contribution_values(scaled_contributions, 0.9) \n",
    "\n",
    "    contribution_idx = [el.nonzero(as_tuple=True)[0] for el in top_x_contributions]\n",
    "    contribution_values = [top_x_contributions[i][idx] for i, idx in enumerate(contribution_idx)]\n",
    "\n",
    "    return contribution_idx, contribution_values\n",
    "\n",
    "def get_direct_contributions(direct_parts: torch.Tensor, concept_vectors: torch.Tensor, module_lookup: List[str], target_module: str, target_token_pos: List[int], batch_size: int = 16) -> DataDict:\n",
    "    num_examples = len(target_token_pos)\n",
    "    active_tasks = get_initial_tasks(concept_vectors, target_module, num_examples, target_token_pos)\n",
    "\n",
    "    dl = DataLoader(active_tasks, batch_size=batch_size, collate_fn=TensorCollator(), shuffle=False)\n",
    "    direct_contributions = DataDict()\n",
    "\n",
    "    for batch in batch_iterator(dl):\n",
    "        contribution_idx, contribution_values = get_contributions(batch, direct_parts)\n",
    "        contribution_modules = [[module_lookup[id] for id in idx] for idx in contribution_idx]\n",
    "\n",
    "        direct_contributions.extend({\n",
    "            'contribution_idx': contribution_idx,\n",
    "            'contribution_values':contribution_values,\n",
    "            'contribution_modules': contribution_modules,\n",
    "            'target_sample_ids': batch['target_sample_ids'],\n",
    "            'target_token_pos': batch['target_token_pos'],\n",
    "            'target_idx': batch['target_idx'],\n",
    "            'depth': batch['depth'],\n",
    "            })\n",
    "            \n",
    "    return direct_contributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "684c880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def update_task_queue(task_queue: DataDict, new_contributions: DataDict) -> DataDict:\n",
    "    for cache_item in new_contributions.to_list():\n",
    "        contribution_idx = cache_item['contribution_idx']\n",
    "\n",
    "        non_emb_contribution_ids = [i for i, idx in enumerate(contribution_idx) if idx != 0]\n",
    "        num_new_tasks = len(non_emb_contribution_ids)\n",
    "\n",
    "        if not num_new_tasks: continue\n",
    "\n",
    "        non_emb_contribution_idx = [contribution_idx[i] for i in non_emb_contribution_ids]\n",
    "        non_emb_contribution_values = [cache_item['contribution_values'][i] for i in non_emb_contribution_ids]\n",
    "        non_emb_target_modules = [cache_item['contribution_modules'][i] for i in non_emb_contribution_ids]\n",
    "\n",
    "        task_queue.extend({\n",
    "            'target_idx': non_emb_contribution_idx,\n",
    "            'target_values': non_emb_contribution_values,\n",
    "            'target_modules': non_emb_target_modules,\n",
    "            'target_sample_ids': [cache_item['target_sample_ids']] * num_new_tasks,\n",
    "            'source_token_pos': [cache_item['target_token_pos']] * num_new_tasks,\n",
    "            'depth': [[depth + 1 for depth in cache_item['depth']]] * num_new_tasks\n",
    "        })\n",
    "    task_queue = merge_task_queue_items(task_queue)\n",
    "\n",
    "    return task_queue\n",
    "\n",
    "def merge_task_queue_items(task_queue: DataDict) -> DataDict:\n",
    "    task_queue.sort(by='target_sample_ids')\n",
    "    task_queue.sort(by='target_idx', descending=True)\n",
    "\n",
    "    prev_task = task_queue[0]\n",
    "    merge_idx = []\n",
    "    value_accumulator = [[]]\n",
    "    acc_depth_list = [[]]\n",
    "\n",
    "    for i, task in enumerate(task_queue.to_list()):\n",
    "        if task['target_sample_ids'] == prev_task['target_sample_ids'] \\\n",
    "            and task['target_idx'] == prev_task['target_idx']:\n",
    "            \n",
    "            value_accumulator[-1].append(task['target_values'])\n",
    "            acc_depth_list[-1].append(task['depth'][0])\n",
    "            continue\n",
    "        \n",
    "        merge_idx.append(i - 1)\n",
    "        prev_task = task\n",
    "        value_accumulator.append([task['target_values']])\n",
    "        acc_depth_list.append([task['depth'][0]])\n",
    "    merge_idx.append(i - 1)\n",
    "\n",
    "    merged_tasks = task_queue[merge_idx]\n",
    "    merged_tasks['target_values'] = [sum(task_values) for task_values in value_accumulator]\n",
    "    merged_tasks['depth'] = [sorted(acc_depth) for acc_depth in acc_depth_list]\n",
    "\n",
    "    return merged_tasks\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_active_tasks(llm: PreTrainedModel, task_queue: DataDict, projection_cache: Dict) -> Tuple[DataDict, DataDict]:\n",
    "    task_queue.sort(by='target_modules', descending=True)\n",
    "    next_module_type = '.'.join(task_queue[0]['target_modules'].split('.')[:2])\n",
    "    end_idx = next((i for i, module in enumerate(task_queue['target_modules']) if not module.startswith(next_module_type)), None)\n",
    "    \n",
    "    if not end_idx:\n",
    "        new_active_tasks = task_queue\n",
    "        new_task_queue = DataDict.from_dict({k:[None] for k in task_queue})\n",
    "    else:\n",
    "        new_active_tasks = task_queue[:end_idx]\n",
    "        new_task_queue = task_queue[end_idx:]\n",
    "    \n",
    "    target_token_pos = get_target_token_pos(new_active_tasks)\n",
    "    new_active_tasks.attach('target_token_pos', target_token_pos)\n",
    "\n",
    "    target_vectors = get_target_vectors(llm, new_active_tasks, projection_cache)\n",
    "    target_contribution_factor = get_contribution_factor(llm, new_active_tasks, projection_cache)\n",
    "\n",
    "    new_active_tasks.attach('target_vectors', target_vectors)\n",
    "    new_active_tasks.attach('target_contribution_factor', target_contribution_factor)\n",
    "\n",
    "    return new_active_tasks, new_task_queue\n",
    "\n",
    "def get_target_vectors(llm: PreTrainedModel, tasks: DataDict, projection_cache: Dict) -> List[torch.Tensor]:\n",
    "\n",
    "    target_layer, module_type = tasks[0]['target_modules'].split('.')[:2]\n",
    "    target_heads = [int(module.split('.')[-1]) for module in tasks['target_modules']]\n",
    "\n",
    "    sample_ids = tasks['target_sample_ids']\n",
    "    token_pos = tasks['target_token_pos']\n",
    "    if module_type == 'attn':\n",
    "        target_vectors = get_attn_target_vectors(llm, projection_cache, int(target_layer), sample_ids, token_pos, target_heads)\n",
    "        return target_vectors\n",
    "    if module_type == 'mlp':\n",
    "        target_vectors = get_mlp_target_vectors(llm, projection_cache, int(target_layer), sample_ids, token_pos, target_heads)\n",
    "        return target_vectors\n",
    "    raise ValueError(f\"Unexpected module type: {module_type}\")\n",
    "\n",
    "def get_attn_target_vectors(llm: PreTrainedModel, projection_cache: Dict, target_layer: int, sample_ids: List[int], token_pos: List[int], target_heads: List[int]) -> torch.Tensor:\n",
    "    num_attn_heads = llm.config.num_attention_heads\n",
    "    num_k_v_heads = llm.config.num_key_value_heads\n",
    "    num_head_groups = num_attn_heads // num_k_v_heads\n",
    "    head_dim = llm.config.head_dim\n",
    "    hidden_dim = llm.config.hidden_size\n",
    "\n",
    "    k_v_head_pos = torch.tensor(target_heads) // head_dim // num_head_groups\n",
    "    neuron_pos = torch.tensor(target_heads) % head_dim\n",
    "\n",
    "    v_proj_W = llm.layers[target_layer].self_attn.v_proj.weight.data.view(num_k_v_heads, head_dim, hidden_dim)[(k_v_head_pos, neuron_pos)]\n",
    "    norm_W = llm.layers[target_layer].input_layernorm.weight.data\n",
    "    norm_var = projection_cache[f\"{target_layer}.attn_norm_var\"][(sample_ids, token_pos)]\n",
    "\n",
    "    target_vectors = trace_through_layer_norm(v_proj_W, norm_W, norm_var)\n",
    "\n",
    "    return target_vectors.cpu()\n",
    "\n",
    "def get_mlp_target_vectors(llm: PreTrainedModel, projection_cache: Dict, target_layer: int, sample_ids: List[int], token_pos: List[int], target_heads: List[int]) -> torch.Tensor:\n",
    "    up_proj_W = llm.layers[target_layer].mlp.up_proj.weight.data[target_heads]\n",
    "    norm_W = llm.layers[target_layer].post_attention_layernorm.weight.data\n",
    "    norm_var = projection_cache[f\"{target_layer}.mlp_norm_var\"][sample_ids, token_pos]\n",
    "\n",
    "    target_vectors = trace_through_layer_norm(up_proj_W, norm_W, norm_var)\n",
    "\n",
    "    return target_vectors.cpu()\n",
    "\n",
    "@torch.no_grad()\n",
    "def trace_through_layer_norm(target: torch.Tensor, norm_W: torch.Tensor, norm_var: torch.Tensor, device: Union[str, torch.device] = 'cuda') -> torch.Tensor:\n",
    "    input_device = target.device\n",
    "    if not device:\n",
    "        device = input_device\n",
    "    \n",
    "    var_scaled_target = target.to(device) * norm_var[:, None].to(device)\n",
    "    norm_w_scaled_target = var_scaled_target / norm_W.to(device)\n",
    "\n",
    "    return norm_w_scaled_target.to(input_device)\n",
    "\n",
    "def get_contribution_factor(llm: PreTrainedModel, tasks: DataDict, projection_cache: Dict):\n",
    "    target_layer, module_type = tasks[0]['target_modules'].split('.')[:2]\n",
    "    target_heads = [int(module.split('.')[-1]) for module in tasks['target_modules']]\n",
    "\n",
    "    sample_ids = tasks['target_sample_ids']\n",
    "    target_values = torch.tensor(tasks['target_values'])\n",
    "    token_pos = tasks['target_token_pos']\n",
    "    if module_type == 'attn':\n",
    "        contribution_factor = get_attn_contribution_factor(llm, target_values, projection_cache, int(target_layer), sample_ids, token_pos, target_heads)\n",
    "        return contribution_factor\n",
    "    if module_type == 'mlp':\n",
    "        contribution_factor = get_mlp_contribution_factor(target_values, projection_cache, int(target_layer), sample_ids, token_pos, target_heads)\n",
    "        return contribution_factor\n",
    "    raise ValueError(f\"Unexpected module type: {module_type}\")\n",
    "\n",
    "def get_attn_contribution_factor(llm: PreTrainedModel, target_values: torch.Tensor, projection_cache: Dict, target_layer: int, sample_ids: List[int], token_pos: List[int], target_heads: List[int]) -> torch.Tensor:\n",
    "    num_attn_heads = llm.config.num_attention_heads\n",
    "    num_k_v_heads = llm.config.num_key_value_heads\n",
    "    num_head_groups = num_attn_heads // num_k_v_heads\n",
    "    head_dim = llm.config.head_dim\n",
    "    num_targets = len(sample_ids)\n",
    "\n",
    "    k_v_head_pos = torch.tensor(target_heads) // head_dim // num_head_groups\n",
    "    neuron_pos = torch.tensor(target_heads) % head_dim\n",
    "\n",
    "    v_proj = projection_cache[f\"{target_layer}.v_proj\"][sample_ids, token_pos].view(num_targets, num_k_v_heads, head_dim)[range(num_targets), k_v_head_pos, neuron_pos]\n",
    "    contribution_factor = target_values / v_proj\n",
    "    print(contribution_factor, target_values, v_proj)\n",
    "\n",
    "    return contribution_factor\n",
    "\n",
    "def get_mlp_contribution_factor(target_values: torch.Tensor, projection_cache: Dict, target_layer: int, sample_ids: List[int], token_pos: List[int], target_heads: List[int]) -> torch.Tensor:\n",
    "    up_proj =  projection_cache[f\"{target_layer}.up_proj\"][sample_ids, token_pos, target_heads]\n",
    "    contribution_factor = target_values / up_proj\n",
    "    print(contribution_factor, target_values, up_proj)\n",
    "\n",
    "    return contribution_factor\n",
    "\n",
    "def get_target_token_pos(tasks: DataDict):\n",
    "    module_token_pos = torch.tensor([int(mod.split('.')[-2]) for mod in tasks['target_modules']])\n",
    "    source_token_pos = torch.tensor(tasks['source_token_pos'])\n",
    "    target_token_pos = torch.where(module_token_pos != -1, module_token_pos, source_token_pos).tolist()\n",
    "\n",
    "    return target_token_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "effe3fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flatten_contributions(contributions: DataDict) -> DataDict:\n",
    "    flat_contributions = DataDict()\n",
    "    if not contributions.length: return flat_contributions\n",
    "    for cache_item in contributions.to_list():\n",
    "        num_contributions = len(cache_item['contribution_idx'])\n",
    "\n",
    "        flat_contributions.extend({\n",
    "            'contribution_idx': cache_item['contribution_idx'].tolist(),\n",
    "            'contribution_values': cache_item['contribution_values'].tolist(),\n",
    "            'contribution_modules': cache_item['contribution_modules'],\n",
    "            'sample_ids': [cache_item['target_sample_ids']] * num_contributions,\n",
    "            'source_idx': [cache_item['target_idx'].item() if cache_item['target_idx'] else None] * num_contributions,\n",
    "            'source_token_pos': [cache_item['target_token_pos']] * num_contributions,\n",
    "            'depth': [cache_item['depth']] * num_contributions\n",
    "        })\n",
    "    return flat_contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecf6b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(sample_id):\n",
    "    no_layers = 10\n",
    "    data_dir = f'/raid/dacslab/CONCEPT_FORMATION/homograph_small/Qwen_Qwen3-4B/{sample_id}/'\n",
    "\n",
    "    data_keys = ['ent_pos_idx', '9.post']\n",
    "    target_token_res_states = TransformerCache.load(data_dir, data_keys, model_id=model_ids[0])\n",
    "    target_token_res_states.stack_tensors()\n",
    "\n",
    "    num_homographs = 1\n",
    "    num_examples = 5\n",
    "\n",
    "    concept_vectors = {}\n",
    "    post_pairs = target_token_res_states[f'9.post'].view(num_homographs, 2, num_examples, -1)\n",
    "    concept_vectors[f'9.post'] = get_mass_mean_vectors(post_pairs)\n",
    "\n",
    "    target_token_pos = target_token_res_states['ent_pos_idx']\n",
    "\n",
    "    del target_token_res_states\n",
    "    gc.collect()\n",
    "\n",
    "    target_module = '9.post'\n",
    "    target_layer = 10\n",
    "    batch_size = 32\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_ids[0])\n",
    "    module_lookup = get_module_lookup(config, target_layer, num_tokens)\n",
    "\n",
    "    direct_contributions = get_direct_contributions(direct_parts, concept_vectors, module_lookup, target_module, target_token_pos)\n",
    "\n",
    "    del direct_parts\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    # get projection cache\n",
    "    data_keys = [f\"{layer}.{module}\" for layer in range(num_layers) for module in ['v_proj', 'attn_norm_var', 'up_proj', 'mlp_norm_var']]\n",
    "    projection_cache = TransformerCache.load(data_dir, data_keys, model_id=model_ids[0])\n",
    "    projection_cache.stack_tensors(padding=True)\n",
    "\n",
    "\n",
    "    # free memory \n",
    "    del model_cache\n",
    "    gc.collect()\n",
    "\n",
    "    task_queue = update_task_queue(DataDict(), direct_contributions)\n",
    "    active_tasks, task_queue = get_active_tasks(llm, task_queue, projection_cache)\n",
    "\n",
    "\n",
    "    with load_language_model(\n",
    "        model_id=model_ids[0],\n",
    "        trust_remote_code=True,\n",
    "        device_map='auto',\n",
    "        dtype=torch.bfloat16,\n",
    "        attn_implementation = 'eager'\n",
    "    ) as (llm, config):\n",
    "        \n",
    "        target_module = '9.post'\n",
    "        target_layer = 10\n",
    "        batch_size = 32\n",
    "\n",
    "        indirect_contributions = []\n",
    "        while active_tasks.length:\n",
    "            print(active_tasks['target_modules'][0])\n",
    "            indirect_contributions.append(DataDict())\n",
    "            dl = DataLoader(active_tasks, batch_size=batch_size, collate_fn=TensorCollator(), shuffle=False)\n",
    "            \n",
    "            for batch in batch_iterator(dl):\n",
    "                target_module = batch['target_modules'][0]\n",
    "                contribution_ceiling_id = get_contribution_ceiling_id(target_module, module_lookup)\n",
    "                batch_parts = get_batch_parts(batch, indirect_parts_cache, contribution_ceiling_id)\n",
    "        \n",
    "                contribution_idx, contribution_values = get_contributions(batch, batch_parts)\n",
    "                contribution_modules = [[module_lookup[id] for id in idx] for idx in contribution_idx]\n",
    "                indirect_contributions[-1].extend({\n",
    "                    'contribution_idx': contribution_idx,\n",
    "                    'contribution_values':contribution_values,\n",
    "                    'contribution_modules': contribution_modules,\n",
    "                    'target_sample_ids': batch['target_sample_ids'],\n",
    "                    'target_token_pos': batch['target_token_pos'],\n",
    "                    'target_modules': batch['target_modules'],\n",
    "                    'target_idx': batch['target_idx'],\n",
    "                    'depth': batch['depth'],\n",
    "                    })\n",
    "            if target_module.startswith('0.attn'):\n",
    "                break\n",
    "            task_queue = update_task_queue(task_queue, indirect_contributions[-1])\n",
    "            active_tasks, task_queue = get_active_tasks(llm, task_queue, projection_cache)\n",
    "\n",
    "            direct_flat_contributions = flatten_contributions(direct_contributions)\n",
    "\n",
    "    raise\n",
    "    df = pd.DataFrame(data=direct_flat_contributions.to_dict())\n",
    "\n",
    "    for i, contribution_cache in enumerate(indirect_contributions):\n",
    "        flat_contributions = flatten_contributions(contribution_cache)\n",
    "        if not flat_contributions.length: continue\n",
    "        for batch_start in range(0, len(flat_contributions), 2000):\n",
    "            contribution_df = pd.DataFrame(data=flat_contributions[batch_start: batch_start + 2000].to_dict())\n",
    "            df = pd.concat([df, contribution_df])\n",
    "\n",
    "    df['layer'] = df['contribution_modules'].apply(lambda x: int(x.split('.')[0]) if x != 'emb' else 0)\n",
    "    df['module_type'] = pd.Categorical(df['contribution_modules'].apply(lambda x: x.split('.')[1] if x != 'emb' else 'emb'), categories=['emb', 'attn', 'mlp'], ordered=True)\n",
    "    df['token_pos'] = df.apply(lambda x: int(x['contribution_modules'].split('.')[2]) if x['contribution_modules'] != 'emb' and int(x['contribution_modules'].split('.')[2]) != -1  else x['source_token_pos'], axis=1)\n",
    "    df['head_id'] = df['contribution_modules'].apply(lambda x: int(x.split('.')[3]) if x != 'emb' else 0)\n",
    "    df = df.drop('contribution_modules', axis=1)\n",
    "        \n",
    "    data_path = os.path.join(project_root, f'data/contribution_cache/cache_9_post_with_norm_hom_{sample_id}.parquet')\n",
    "    df.to_parquet(data_path, index=False)\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a31265",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "sample_id = 0\n",
    "num_layers = 10\n",
    "data_dir = f'/raid/dacslab/CONCEPT_FORMATION/homograph_small/Qwen_Qwen3-4B/{sample_id}/'\n",
    "out_dir = f'/raid/dacslab/CONCEPT_FORMATION/homograph_small/Qwen_Qwen3-4B/.cache/{sample_id}/'\n",
    "\n",
    "# load model cache\n",
    "data_keys = ['ent_pos_idx', 'full_emb'] + [f\"{layer}.{module}\" for layer in range(num_layers) for module in ['d_attn', 'd_mlp']]\n",
    "model_cache = TransformerCache.load(data_dir, data_keys, model_id=model_ids[0])\n",
    "model_cache.stack_tensors(padding=True)\n",
    "\n",
    "num_examples = model_cache.length\n",
    "num_tokens = model_cache['0.d_attn'].size(1)\n",
    "target_token_pos = model_cache['ent_pos_idx']\n",
    "batch_ent_idx = (range(model_cache.length), target_token_pos)\n",
    "\n",
    "# get parts for direct contributions\n",
    "direct_parts = [model_cache['full_emb'][batch_ent_idx][:, None]]\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    direct_parts.append(model_cache[f'{layer}.d_attn'][batch_ent_idx].flatten(1, -2))\n",
    "    direct_parts.append(model_cache[f\"{layer}.d_mlp\"][batch_ent_idx])\n",
    "direct_parts = torch.concat(direct_parts, dim=1).transpose(0,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848fd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sample_id = 0\n",
    "num_layers = 10\n",
    "data_dir = f'/raid/dacslab/CONCEPT_FORMATION/homograph_small/Qwen_Qwen3-4B/{sample_id}/'\n",
    "out_dir = f'/raid/dacslab/CONCEPT_FORMATION/homograph_small/Qwen_Qwen3-4B/.cache/{sample_id}/'\n",
    "\n",
    "# load model cache\n",
    "data_keys = ['full_emb'] + [f\"{layer}.{module}\" for layer in range(num_layers) for module in ['d_attn', 'd_mlp']]\n",
    "model_cache = TransformerCache.load(data_dir, data_keys, model_id=model_ids[0])\n",
    "model_cache.stack_tensors(padding=True)\n",
    "print('done loading')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93cc755",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_emb_cont = model_cache['full_emb'][:, :, None].shape[-2]\n",
    "num_attn_cont = model_cache[f'0.d_attn'].flatten(2, -2).shape[-2]\n",
    "num_mlp_cont = model_cache[f\"0.d_mlp\"].shape[-2]\n",
    "total_cont = num_emb_cont + num_mlp_cont * num_layers + num_attn_cont * num_layers\n",
    "\n",
    "num_example, num_token, _, model_dim = model_cache['full_emb'][:, :, None].shape\n",
    "\n",
    "indirect_parts_cache = torch.empty((num_example, num_token, total_cont, model_dim))\n",
    "\n",
    "curr_start = 0\n",
    "# get indirect parts cache\n",
    "indirect_parts_cache[:, :, curr_start:num_emb_cont] = mean_shift(model_cache['full_emb'])[:, :, None]\n",
    "curr_start += num_emb_cont\n",
    "del model_cache.data[f'full_emb']\n",
    "gc.collect()\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    indirect_parts_cache[:, :, curr_start: curr_start + num_attn_cont] = mean_shift(model_cache[f'{layer}.d_attn']).flatten(2, -2)\n",
    "    curr_start += num_attn_cont\n",
    "    del model_cache.data[f'{layer}.d_attn']\n",
    "    gc.collect()\n",
    "\n",
    "    indirect_parts_cache[:, :, curr_start: curr_start + num_mlp_cont] = mean_shift(model_cache[f\"{layer}.d_mlp\"])\n",
    "    curr_start += num_mlp_cont\n",
    "    del model_cache.data[f'{layer}.d_mlp']\n",
    "    gc.collect()\n",
    "\n",
    "print('done adding')\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "torch.save(indirect_parts_cache, os.path.join(out_dir, 'indirect_parts.safetensor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96247e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m      2\u001b[39m  \u001b[38;5;66;03m# get indirect parts cache\u001b[39;00m\n\u001b[32m      3\u001b[39m indirect_parts_cache = [mean_shift(model_cache[\u001b[33m'\u001b[39m\u001b[33mfull_emb\u001b[39m\u001b[33m'\u001b[39m])[:, :, \u001b[38;5;28;01mNone\u001b[39;00m]]\n",
      "\u001b[31mRuntimeError\u001b[39m: No active exception to reraise"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15621fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/1: 2.27 seconds\n"
     ]
    }
   ],
   "source": [
    "raise \n",
    "main(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442953c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad46801b27ac48d0ad6744db1495f668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/1: 1.63 seconds\n",
      "9.mlp.-1.9705\n",
      "Batch 1/5: 7.25 seconds\n",
      "Batch 2/5: 9.65 seconds\n",
      "Batch 3/5: 9.09 seconds\n",
      "Batch 4/5: 9.47 seconds\n",
      "Batch 5/5: 5.69 seconds\n",
      "9.attn.3.2710\n",
      "Batch 1/1: 5.35 seconds\n",
      "8.mlp.-1.9668\n",
      "Batch 1/6: 8.60 seconds\n",
      "Batch 2/6: 8.60 seconds\n",
      "Batch 3/6: 9.00 seconds\n",
      "Batch 4/6: 9.06 seconds\n",
      "Batch 5/6: 8.42 seconds\n",
      "Batch 6/6: 2.71 seconds\n",
      "8.attn.4.2033\n",
      "Batch 1/1: 2.52 seconds\n",
      "7.mlp.-1.9679\n",
      "Batch 1/7: 6.39 seconds\n",
      "Batch 2/7: 7.23 seconds\n",
      "Batch 3/7: 7.41 seconds\n",
      "Batch 4/7: 7.21 seconds\n",
      "Batch 5/7: 7.29 seconds\n",
      "Batch 6/7: 7.24 seconds\n",
      "Batch 7/7: 4.82 seconds\n",
      "7.attn.4.1487\n",
      "Batch 1/1: 5.71 seconds\n",
      "6.mlp.-1.9695\n",
      "Batch 1/9: 6.14 seconds\n",
      "Batch 2/9: 6.03 seconds\n",
      "Batch 3/9: 6.40 seconds\n",
      "Batch 4/9: 6.39 seconds\n",
      "Batch 5/9: 6.26 seconds\n",
      "Batch 6/9: 6.21 seconds\n",
      "Batch 7/9: 6.00 seconds\n",
      "Batch 8/9: 6.24 seconds\n",
      "Batch 9/9: 3.98 seconds\n",
      "6.attn.4.1278\n",
      "Batch 1/10: 4.96 seconds\n",
      "Batch 2/10: 5.79 seconds\n",
      "Batch 3/10: 5.49 seconds\n",
      "Batch 4/10: 5.50 seconds\n",
      "Batch 5/10: 5.49 seconds\n",
      "Batch 6/10: 7.21 seconds\n",
      "Batch 7/10: 5.79 seconds\n",
      "Batch 8/10: 6.93 seconds\n",
      "Batch 9/10: 5.74 seconds\n",
      "Batch 10/10: 3.86 seconds\n",
      "5.mlp.-1.991\n",
      "Batch 1/43: 5.40 seconds\n",
      "Batch 2/43: 5.58 seconds\n",
      "Batch 3/43: 5.58 seconds\n",
      "Batch 4/43: 5.44 seconds\n",
      "Batch 5/43: 5.09 seconds\n",
      "Batch 6/43: 5.38 seconds\n",
      "Batch 7/43: 5.27 seconds\n",
      "Batch 8/43: 5.21 seconds\n",
      "Batch 9/43: 5.22 seconds\n",
      "Batch 10/43: 5.30 seconds\n",
      "Batch 11/43: 5.10 seconds\n",
      "Batch 12/43: 5.07 seconds\n",
      "Batch 13/43: 5.21 seconds\n",
      "Batch 14/43: 5.21 seconds\n",
      "Batch 15/43: 5.08 seconds\n",
      "Batch 16/43: 5.01 seconds\n",
      "Batch 17/43: 5.09 seconds\n",
      "Batch 18/43: 5.08 seconds\n",
      "Batch 19/43: 5.20 seconds\n",
      "Batch 20/43: 5.54 seconds\n",
      "Batch 21/43: 5.09 seconds\n",
      "Batch 22/43: 5.09 seconds\n",
      "Batch 23/43: 5.02 seconds\n",
      "Batch 24/43: 5.25 seconds\n",
      "Batch 25/43: 5.01 seconds\n",
      "Batch 26/43: 4.97 seconds\n",
      "Batch 27/43: 5.11 seconds\n",
      "Batch 28/43: 4.99 seconds\n",
      "Batch 29/43: 5.27 seconds\n",
      "Batch 30/43: 5.02 seconds\n",
      "Batch 31/43: 5.01 seconds\n",
      "Batch 32/43: 4.98 seconds\n",
      "Batch 33/43: 4.95 seconds\n",
      "Batch 34/43: 5.04 seconds\n",
      "Batch 35/43: 5.08 seconds\n",
      "Batch 36/43: 5.06 seconds\n",
      "Batch 37/43: 5.42 seconds\n",
      "Batch 38/43: 5.27 seconds\n",
      "Batch 39/43: 5.32 seconds\n",
      "Batch 40/43: 5.26 seconds\n",
      "Batch 41/43: 4.94 seconds\n",
      "Batch 42/43: 5.01 seconds\n",
      "Batch 43/43: 2.21 seconds\n",
      "5.attn.4.981\n",
      "Batch 1/26: 3.50 seconds\n",
      "Batch 2/26: 4.72 seconds\n",
      "Batch 3/26: 4.55 seconds\n",
      "Batch 4/26: 4.46 seconds\n",
      "Batch 5/26: 4.65 seconds\n",
      "Batch 6/26: 4.55 seconds\n",
      "Batch 7/26: 4.20 seconds\n",
      "Batch 8/26: 4.89 seconds\n",
      "Batch 9/26: 4.78 seconds\n",
      "Batch 10/26: 4.85 seconds\n",
      "Batch 11/26: 4.31 seconds\n",
      "Batch 12/26: 4.43 seconds\n",
      "Batch 13/26: 4.35 seconds\n",
      "Batch 14/26: 4.46 seconds\n",
      "Batch 15/26: 4.54 seconds\n",
      "Batch 16/26: 4.39 seconds\n",
      "Batch 17/26: 4.74 seconds\n",
      "Batch 18/26: 4.84 seconds\n",
      "Batch 19/26: 4.24 seconds\n",
      "Batch 20/26: 4.97 seconds\n",
      "Batch 21/26: 5.12 seconds\n",
      "Batch 22/26: 4.80 seconds\n",
      "Batch 23/26: 4.35 seconds\n",
      "Batch 24/26: 4.54 seconds\n",
      "Batch 25/26: 4.28 seconds\n",
      "Batch 26/26: 4.34 seconds\n",
      "4.mlp.-1.999\n",
      "Batch 1/76: 4.69 seconds\n",
      "Batch 2/76: 4.27 seconds\n",
      "Batch 3/76: 3.88 seconds\n",
      "Batch 4/76: 4.39 seconds\n",
      "Batch 5/76: 4.01 seconds\n",
      "Batch 6/76: 4.31 seconds\n",
      "Batch 7/76: 3.98 seconds\n",
      "Batch 8/76: 4.14 seconds\n",
      "Batch 9/76: 4.04 seconds\n",
      "Batch 10/76: 4.36 seconds\n",
      "Batch 11/76: 3.93 seconds\n",
      "Batch 12/76: 4.16 seconds\n",
      "Batch 13/76: 4.11 seconds\n",
      "Batch 14/76: 4.19 seconds\n",
      "Batch 15/76: 4.11 seconds\n",
      "Batch 16/76: 4.49 seconds\n",
      "Batch 17/76: 4.10 seconds\n",
      "Batch 18/76: 4.33 seconds\n",
      "Batch 19/76: 4.10 seconds\n",
      "Batch 20/76: 4.20 seconds\n",
      "Batch 21/76: 4.00 seconds\n",
      "Batch 22/76: 4.13 seconds\n",
      "Batch 23/76: 4.18 seconds\n",
      "Batch 24/76: 4.33 seconds\n",
      "Batch 25/76: 4.16 seconds\n",
      "Batch 26/76: 4.16 seconds\n",
      "Batch 27/76: 3.92 seconds\n",
      "Batch 28/76: 4.34 seconds\n",
      "Batch 29/76: 4.03 seconds\n",
      "Batch 30/76: 4.33 seconds\n",
      "Batch 31/76: 4.12 seconds\n",
      "Batch 32/76: 4.44 seconds\n",
      "Batch 33/76: 3.98 seconds\n",
      "Batch 34/76: 4.37 seconds\n",
      "Batch 35/76: 4.31 seconds\n",
      "Batch 36/76: 4.51 seconds\n",
      "Batch 37/76: 4.17 seconds\n",
      "Batch 38/76: 4.29 seconds\n",
      "Batch 39/76: 4.56 seconds\n",
      "Batch 40/76: 4.39 seconds\n",
      "Batch 41/76: 4.31 seconds\n",
      "Batch 42/76: 4.35 seconds\n",
      "Batch 43/76: 4.26 seconds\n",
      "Batch 44/76: 4.49 seconds\n",
      "Batch 45/76: 4.38 seconds\n",
      "Batch 46/76: 4.75 seconds\n",
      "Batch 47/76: 4.12 seconds\n",
      "Batch 48/76: 4.16 seconds\n",
      "Batch 49/76: 4.08 seconds\n",
      "Batch 50/76: 4.20 seconds\n",
      "Batch 51/76: 4.11 seconds\n",
      "Batch 52/76: 4.38 seconds\n",
      "Batch 53/76: 4.34 seconds\n",
      "Batch 54/76: 4.45 seconds\n",
      "Batch 55/76: 4.14 seconds\n",
      "Batch 56/76: 4.66 seconds\n",
      "Batch 57/76: 4.10 seconds\n",
      "Batch 58/76: 4.14 seconds\n",
      "Batch 59/76: 4.56 seconds\n",
      "Batch 60/76: 4.27 seconds\n",
      "Batch 61/76: 4.31 seconds\n",
      "Batch 62/76: 4.13 seconds\n",
      "Batch 63/76: 4.24 seconds\n",
      "Batch 64/76: 4.22 seconds\n",
      "Batch 65/76: 4.30 seconds\n",
      "Batch 66/76: 4.62 seconds\n",
      "Batch 67/76: 4.39 seconds\n",
      "Batch 68/76: 4.40 seconds\n",
      "Batch 69/76: 4.17 seconds\n",
      "Batch 70/76: 4.28 seconds\n",
      "Batch 71/76: 4.23 seconds\n",
      "Batch 72/76: 4.54 seconds\n",
      "Batch 73/76: 4.90 seconds\n",
      "Batch 74/76: 4.45 seconds\n",
      "Batch 75/76: 4.03 seconds\n",
      "Batch 76/76: 3.03 seconds\n",
      "4.attn.4.1529\n",
      "Batch 1/20: 3.12 seconds\n",
      "Batch 2/20: 3.64 seconds\n",
      "Batch 3/20: 3.70 seconds\n",
      "Batch 4/20: 4.02 seconds\n",
      "Batch 5/20: 3.70 seconds\n",
      "Batch 6/20: 4.00 seconds\n",
      "Batch 7/20: 3.67 seconds\n",
      "Batch 8/20: 3.92 seconds\n",
      "Batch 9/20: 3.75 seconds\n",
      "Batch 10/20: 4.07 seconds\n",
      "Batch 11/20: 3.74 seconds\n",
      "Batch 12/20: 3.81 seconds\n",
      "Batch 13/20: 3.78 seconds\n",
      "Batch 14/20: 4.16 seconds\n",
      "Batch 15/20: 4.20 seconds\n",
      "Batch 16/20: 4.53 seconds\n",
      "Batch 17/20: 3.99 seconds\n",
      "Batch 18/20: 4.00 seconds\n",
      "Batch 19/20: 3.92 seconds\n",
      "Batch 20/20: 2.69 seconds\n",
      "3.mlp.-1.999\n",
      "Batch 1/53: 3.27 seconds\n",
      "Batch 2/53: 3.70 seconds\n",
      "Batch 3/53: 3.59 seconds\n",
      "Batch 4/53: 3.64 seconds\n",
      "Batch 5/53: 3.51 seconds\n",
      "Batch 6/53: 3.68 seconds\n",
      "Batch 7/53: 3.53 seconds\n",
      "Batch 8/53: 3.66 seconds\n",
      "Batch 9/53: 3.51 seconds\n",
      "Batch 10/53: 3.54 seconds\n",
      "Batch 11/53: 3.53 seconds\n",
      "Batch 12/53: 3.68 seconds\n",
      "Batch 13/53: 3.58 seconds\n",
      "Batch 14/53: 3.60 seconds\n",
      "Batch 15/53: 3.50 seconds\n",
      "Batch 16/53: 3.68 seconds\n",
      "Batch 17/53: 3.66 seconds\n",
      "Batch 18/53: 3.55 seconds\n",
      "Batch 19/53: 3.49 seconds\n",
      "Batch 20/53: 3.57 seconds\n",
      "Batch 21/53: 3.44 seconds\n",
      "Batch 22/53: 3.46 seconds\n",
      "Batch 23/53: 3.28 seconds\n",
      "Batch 24/53: 3.55 seconds\n",
      "Batch 25/53: 3.42 seconds\n",
      "Batch 26/53: 3.65 seconds\n",
      "Batch 27/53: 3.47 seconds\n",
      "Batch 28/53: 3.51 seconds\n",
      "Batch 29/53: 3.47 seconds\n",
      "Batch 30/53: 3.55 seconds\n",
      "Batch 31/53: 3.44 seconds\n",
      "Batch 32/53: 3.56 seconds\n",
      "Batch 33/53: 3.48 seconds\n",
      "Batch 34/53: 3.55 seconds\n",
      "Batch 35/53: 3.48 seconds\n",
      "Batch 36/53: 3.39 seconds\n",
      "Batch 37/53: 3.38 seconds\n",
      "Batch 38/53: 3.48 seconds\n",
      "Batch 39/53: 3.42 seconds\n",
      "Batch 40/53: 3.59 seconds\n",
      "Batch 41/53: 3.38 seconds\n",
      "Batch 42/53: 3.51 seconds\n",
      "Batch 43/53: 3.42 seconds\n",
      "Batch 44/53: 3.58 seconds\n",
      "Batch 45/53: 3.39 seconds\n",
      "Batch 46/53: 3.53 seconds\n",
      "Batch 47/53: 3.49 seconds\n",
      "Batch 48/53: 3.64 seconds\n",
      "Batch 49/53: 3.35 seconds\n",
      "Batch 50/53: 3.55 seconds\n",
      "Batch 51/53: 3.51 seconds\n",
      "Batch 52/53: 3.63 seconds\n",
      "Batch 53/53: 3.24 seconds\n",
      "3.attn.4.1231\n",
      "Batch 1/13: 2.94 seconds\n",
      "Batch 2/13: 2.84 seconds\n",
      "Batch 3/13: 2.98 seconds\n",
      "Batch 4/13: 2.79 seconds\n",
      "Batch 5/13: 2.96 seconds\n",
      "Batch 6/13: 2.82 seconds\n",
      "Batch 7/13: 2.92 seconds\n",
      "Batch 8/13: 2.93 seconds\n",
      "Batch 9/13: 2.89 seconds\n",
      "Batch 10/13: 2.96 seconds\n",
      "Batch 11/13: 2.97 seconds\n",
      "Batch 12/13: 2.88 seconds\n",
      "Batch 13/13: 2.42 seconds\n",
      "2.mlp.-1.987\n",
      "Batch 1/43: 2.42 seconds\n",
      "Batch 2/43: 2.70 seconds\n",
      "Batch 3/43: 2.48 seconds\n",
      "Batch 4/43: 2.68 seconds\n",
      "Batch 5/43: 2.51 seconds\n",
      "Batch 6/43: 2.74 seconds\n",
      "Batch 7/43: 2.52 seconds\n",
      "Batch 8/43: 2.61 seconds\n",
      "Batch 9/43: 2.57 seconds\n",
      "Batch 10/43: 2.62 seconds\n",
      "Batch 11/43: 2.56 seconds\n",
      "Batch 12/43: 2.58 seconds\n",
      "Batch 13/43: 2.55 seconds\n",
      "Batch 14/43: 2.65 seconds\n",
      "Batch 15/43: 2.57 seconds\n",
      "Batch 16/43: 2.65 seconds\n",
      "Batch 17/43: 2.51 seconds\n",
      "Batch 18/43: 2.45 seconds\n",
      "Batch 19/43: 2.39 seconds\n",
      "Batch 20/43: 2.58 seconds\n",
      "Batch 21/43: 2.48 seconds\n",
      "Batch 22/43: 2.52 seconds\n",
      "Batch 23/43: 2.41 seconds\n",
      "Batch 24/43: 2.57 seconds\n",
      "Batch 25/43: 2.59 seconds\n",
      "Batch 26/43: 2.62 seconds\n",
      "Batch 27/43: 2.45 seconds\n",
      "Batch 28/43: 2.52 seconds\n",
      "Batch 29/43: 2.42 seconds\n",
      "Batch 30/43: 2.58 seconds\n",
      "Batch 31/43: 2.55 seconds\n",
      "Batch 32/43: 2.63 seconds\n",
      "Batch 33/43: 2.55 seconds\n",
      "Batch 34/43: 2.64 seconds\n",
      "Batch 35/43: 2.21 seconds\n",
      "Batch 36/43: 2.55 seconds\n",
      "Batch 37/43: 2.51 seconds\n",
      "Batch 38/43: 2.60 seconds\n",
      "Batch 39/43: 2.57 seconds\n",
      "Batch 40/43: 2.56 seconds\n",
      "Batch 41/43: 2.41 seconds\n",
      "Batch 42/43: 2.52 seconds\n",
      "Batch 43/43: 2.13 seconds\n",
      "2.attn.3.3043\n",
      "Batch 1/6: 1.96 seconds\n",
      "Batch 2/6: 1.86 seconds\n",
      "Batch 3/6: 1.86 seconds\n",
      "Batch 4/6: 1.81 seconds\n",
      "Batch 5/6: 1.77 seconds\n",
      "Batch 6/6: 1.64 seconds\n",
      "1.mlp.-1.995\n",
      "Batch 1/36: 1.52 seconds\n",
      "Batch 2/36: 1.48 seconds\n",
      "Batch 3/36: 1.55 seconds\n",
      "Batch 4/36: 1.49 seconds\n",
      "Batch 5/36: 1.57 seconds\n",
      "Batch 6/36: 1.85 seconds\n",
      "Batch 7/36: 1.50 seconds\n",
      "Batch 8/36: 1.51 seconds\n",
      "Batch 9/36: 1.57 seconds\n",
      "Batch 10/36: 1.57 seconds\n",
      "Batch 11/36: 1.60 seconds\n",
      "Batch 12/36: 1.56 seconds\n",
      "Batch 13/36: 1.58 seconds\n",
      "Batch 14/36: 1.53 seconds\n",
      "Batch 15/36: 1.58 seconds\n",
      "Batch 16/36: 1.55 seconds\n",
      "Batch 17/36: 1.61 seconds\n",
      "Batch 18/36: 1.51 seconds\n",
      "Batch 19/36: 1.56 seconds\n",
      "Batch 20/36: 1.51 seconds\n",
      "Batch 21/36: 1.53 seconds\n",
      "Batch 22/36: 1.48 seconds\n",
      "Batch 23/36: 1.56 seconds\n",
      "Batch 24/36: 1.52 seconds\n",
      "Batch 25/36: 1.57 seconds\n",
      "Batch 26/36: 1.54 seconds\n",
      "Batch 27/36: 1.53 seconds\n",
      "Batch 28/36: 1.53 seconds\n",
      "Batch 29/36: 1.49 seconds\n",
      "Batch 30/36: 1.52 seconds\n",
      "Batch 31/36: 1.55 seconds\n",
      "Batch 32/36: 1.51 seconds\n",
      "Batch 33/36: 1.66 seconds\n",
      "Batch 34/36: 1.70 seconds\n",
      "Batch 35/36: 1.73 seconds\n",
      "Batch 36/36: 1.65 seconds\n",
      "1.attn.4.3845\n",
      "Batch 1/20: 1.20 seconds\n",
      "Batch 2/20: 0.95 seconds\n",
      "Batch 3/20: 0.97 seconds\n",
      "Batch 4/20: 0.96 seconds\n",
      "Batch 5/20: 0.92 seconds\n",
      "Batch 6/20: 0.95 seconds\n",
      "Batch 7/20: 0.89 seconds\n",
      "Batch 8/20: 0.92 seconds\n",
      "Batch 9/20: 0.88 seconds\n",
      "Batch 10/20: 0.87 seconds\n",
      "Batch 11/20: 0.96 seconds\n",
      "Batch 12/20: 0.95 seconds\n",
      "Batch 13/20: 0.98 seconds\n",
      "Batch 14/20: 0.95 seconds\n",
      "Batch 15/20: 0.98 seconds\n",
      "Batch 16/20: 0.94 seconds\n",
      "Batch 17/20: 0.98 seconds\n",
      "Batch 18/20: 0.97 seconds\n",
      "Batch 19/20: 0.99 seconds\n",
      "Batch 20/20: 0.77 seconds\n",
      "0.mlp.-1.998\n",
      "Batch 1/92: 0.72 seconds\n",
      "Batch 2/92: 0.66 seconds\n",
      "Batch 3/92: 0.67 seconds\n",
      "Batch 4/92: 0.62 seconds\n",
      "Batch 5/92: 0.65 seconds\n",
      "Batch 6/92: 0.63 seconds\n",
      "Batch 7/92: 0.67 seconds\n",
      "Batch 8/92: 0.64 seconds\n",
      "Batch 9/92: 0.66 seconds\n",
      "Batch 10/92: 0.67 seconds\n",
      "Batch 11/92: 0.68 seconds\n",
      "Batch 12/92: 0.65 seconds\n",
      "Batch 13/92: 0.65 seconds\n",
      "Batch 14/92: 0.66 seconds\n",
      "Batch 15/92: 0.65 seconds\n",
      "Batch 16/92: 0.66 seconds\n",
      "Batch 17/92: 0.70 seconds\n",
      "Batch 18/92: 0.66 seconds\n",
      "Batch 19/92: 0.64 seconds\n",
      "Batch 20/92: 0.65 seconds\n",
      "Batch 21/92: 0.64 seconds\n",
      "Batch 22/92: 0.67 seconds\n",
      "Batch 23/92: 0.69 seconds\n",
      "Batch 24/92: 0.65 seconds\n",
      "Batch 25/92: 0.71 seconds\n",
      "Batch 26/92: 0.65 seconds\n",
      "Batch 27/92: 0.68 seconds\n",
      "Batch 28/92: 0.64 seconds\n",
      "Batch 29/92: 0.65 seconds\n",
      "Batch 30/92: 0.63 seconds\n",
      "Batch 31/92: 0.61 seconds\n",
      "Batch 32/92: 0.63 seconds\n",
      "Batch 33/92: 0.63 seconds\n",
      "Batch 34/92: 0.64 seconds\n",
      "Batch 35/92: 0.63 seconds\n",
      "Batch 36/92: 0.61 seconds\n",
      "Batch 37/92: 0.62 seconds\n",
      "Batch 38/92: 0.65 seconds\n",
      "Batch 39/92: 0.65 seconds\n",
      "Batch 40/92: 0.63 seconds\n",
      "Batch 41/92: 0.64 seconds\n",
      "Batch 42/92: 0.62 seconds\n",
      "Batch 43/92: 0.61 seconds\n",
      "Batch 44/92: 0.62 seconds\n",
      "Batch 45/92: 0.65 seconds\n",
      "Batch 46/92: 0.65 seconds\n",
      "Batch 47/92: 0.63 seconds\n",
      "Batch 48/92: 0.64 seconds\n",
      "Batch 49/92: 0.63 seconds\n",
      "Batch 50/92: 0.65 seconds\n",
      "Batch 51/92: 0.62 seconds\n",
      "Batch 52/92: 0.62 seconds\n",
      "Batch 53/92: 0.63 seconds\n",
      "Batch 54/92: 0.63 seconds\n",
      "Batch 55/92: 0.63 seconds\n",
      "Batch 56/92: 0.62 seconds\n",
      "Batch 57/92: 0.63 seconds\n",
      "Batch 58/92: 0.65 seconds\n",
      "Batch 59/92: 0.65 seconds\n",
      "Batch 60/92: 0.65 seconds\n",
      "Batch 61/92: 0.66 seconds\n",
      "Batch 62/92: 0.65 seconds\n",
      "Batch 63/92: 0.63 seconds\n",
      "Batch 64/92: 0.65 seconds\n",
      "Batch 65/92: 0.66 seconds\n",
      "Batch 66/92: 0.66 seconds\n",
      "Batch 67/92: 0.67 seconds\n",
      "Batch 68/92: 0.65 seconds\n",
      "Batch 69/92: 0.67 seconds\n",
      "Batch 70/92: 0.69 seconds\n",
      "Batch 71/92: 0.69 seconds\n",
      "Batch 72/92: 0.68 seconds\n",
      "Batch 73/92: 0.68 seconds\n",
      "Batch 74/92: 0.71 seconds\n",
      "Batch 75/92: 0.66 seconds\n",
      "Batch 76/92: 0.68 seconds\n",
      "Batch 77/92: 0.68 seconds\n",
      "Batch 78/92: 0.69 seconds\n",
      "Batch 79/92: 0.69 seconds\n",
      "Batch 80/92: 0.68 seconds\n",
      "Batch 81/92: 0.67 seconds\n",
      "Batch 82/92: 0.70 seconds\n",
      "Batch 83/92: 0.70 seconds\n",
      "Batch 84/92: 0.69 seconds\n",
      "Batch 85/92: 0.69 seconds\n",
      "Batch 86/92: 0.72 seconds\n",
      "Batch 87/92: 0.71 seconds\n",
      "Batch 88/92: 0.71 seconds\n",
      "Batch 89/92: 0.71 seconds\n",
      "Batch 90/92: 0.69 seconds\n",
      "Batch 91/92: 0.71 seconds\n",
      "Batch 92/92: 0.56 seconds\n",
      "0.attn.4.764\n",
      "Batch 1/96: 0.12 seconds\n",
      "Batch 2/96: 0.01 seconds\n",
      "Batch 3/96: 0.01 seconds\n",
      "Batch 4/96: 0.01 seconds\n",
      "Batch 5/96: 0.00 seconds\n",
      "Batch 6/96: 0.00 seconds\n",
      "Batch 7/96: 0.00 seconds\n",
      "Batch 8/96: 0.00 seconds\n",
      "Batch 9/96: 0.00 seconds\n",
      "Batch 10/96: 0.00 seconds\n",
      "Batch 11/96: 0.00 seconds\n",
      "Batch 12/96: 0.00 seconds\n",
      "Batch 13/96: 0.00 seconds\n",
      "Batch 14/96: 0.00 seconds\n",
      "Batch 15/96: 0.00 seconds\n",
      "Batch 16/96: 0.00 seconds\n",
      "Batch 17/96: 0.00 seconds\n",
      "Batch 18/96: 0.00 seconds\n",
      "Batch 19/96: 0.00 seconds\n",
      "Batch 20/96: 0.00 seconds\n",
      "Batch 21/96: 0.00 seconds\n",
      "Batch 22/96: 0.00 seconds\n",
      "Batch 23/96: 0.00 seconds\n",
      "Batch 24/96: 0.00 seconds\n",
      "Batch 25/96: 0.00 seconds\n",
      "Batch 26/96: 0.00 seconds\n",
      "Batch 27/96: 0.00 seconds\n",
      "Batch 28/96: 0.00 seconds\n",
      "Batch 29/96: 0.00 seconds\n",
      "Batch 30/96: 0.00 seconds\n",
      "Batch 31/96: 0.00 seconds\n",
      "Batch 32/96: 0.00 seconds\n",
      "Batch 33/96: 0.00 seconds\n",
      "Batch 34/96: 0.00 seconds\n",
      "Batch 35/96: 0.00 seconds\n",
      "Batch 36/96: 0.00 seconds\n",
      "Batch 37/96: 0.00 seconds\n",
      "Batch 38/96: 0.00 seconds\n",
      "Batch 39/96: 0.00 seconds\n",
      "Batch 40/96: 0.00 seconds\n",
      "Batch 41/96: 0.00 seconds\n",
      "Batch 42/96: 0.00 seconds\n",
      "Batch 43/96: 0.00 seconds\n",
      "Batch 44/96: 0.00 seconds\n",
      "Batch 45/96: 0.00 seconds\n",
      "Batch 46/96: 0.00 seconds\n",
      "Batch 47/96: 0.00 seconds\n",
      "Batch 48/96: 0.00 seconds\n",
      "Batch 49/96: 0.00 seconds\n",
      "Batch 50/96: 0.00 seconds\n",
      "Batch 51/96: 0.00 seconds\n",
      "Batch 52/96: 0.00 seconds\n",
      "Batch 53/96: 0.00 seconds\n",
      "Batch 54/96: 0.00 seconds\n",
      "Batch 55/96: 0.00 seconds\n",
      "Batch 56/96: 0.00 seconds\n",
      "Batch 57/96: 0.00 seconds\n",
      "Batch 58/96: 0.00 seconds\n",
      "Batch 59/96: 0.00 seconds\n",
      "Batch 60/96: 0.00 seconds\n",
      "Batch 61/96: 0.00 seconds\n",
      "Batch 62/96: 0.00 seconds\n",
      "Batch 63/96: 0.00 seconds\n",
      "Batch 64/96: 0.00 seconds\n",
      "Batch 65/96: 0.00 seconds\n",
      "Batch 66/96: 0.00 seconds\n",
      "Batch 67/96: 0.00 seconds\n",
      "Batch 68/96: 0.00 seconds\n",
      "Batch 69/96: 0.00 seconds\n",
      "Batch 70/96: 0.00 seconds\n",
      "Batch 71/96: 0.00 seconds\n",
      "Batch 72/96: 0.00 seconds\n",
      "Batch 73/96: 0.00 seconds\n",
      "Batch 74/96: 0.00 seconds\n",
      "Batch 75/96: 0.00 seconds\n",
      "Batch 76/96: 0.00 seconds\n",
      "Batch 77/96: 0.00 seconds\n",
      "Batch 78/96: 0.00 seconds\n",
      "Batch 79/96: 0.00 seconds\n",
      "Batch 80/96: 0.00 seconds\n",
      "Batch 81/96: 0.00 seconds\n",
      "Batch 82/96: 0.00 seconds\n",
      "Batch 83/96: 0.00 seconds\n",
      "Batch 84/96: 0.00 seconds\n",
      "Batch 85/96: 0.00 seconds\n",
      "Batch 86/96: 0.00 seconds\n",
      "Batch 87/96: 0.00 seconds\n",
      "Batch 88/96: 0.00 seconds\n",
      "Batch 89/96: 0.00 seconds\n",
      "Batch 90/96: 0.00 seconds\n",
      "Batch 91/96: 0.00 seconds\n",
      "Batch 92/96: 0.00 seconds\n",
      "Batch 93/96: 0.00 seconds\n",
      "Batch 94/96: 0.00 seconds\n",
      "Batch 95/96: 0.00 seconds\n",
      "Batch 96/96: 0.00 seconds\n",
      "Memory allocated: 0.00 GB\n",
      "Memory reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# for sample_id in range(4, 5):\n",
    "#     main(sample_id)\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacslab_lasse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
