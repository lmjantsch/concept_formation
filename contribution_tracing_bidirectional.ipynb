{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4739c9d",
   "metadata": {},
   "source": [
    "## Task List\n",
    "\n",
    "* Create Graph based version\n",
    "    - Each module is a node\n",
    "    - Each contribution is an edge\n",
    "    - Update Node with for ecach incomming (and outgoing) edge\n",
    "* Change to bi-directional batching\n",
    "    - Start with last token -> load parts -> calculate contribution for each layer -> repeat for previous token until no open nodes are left.\n",
    "    - Less memory intensive (Only load cache for one token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503273e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import json\n",
    "import sys\n",
    "import gc\n",
    "from typing import Dict, List, Union, Tuple, Iterator, Generator\n",
    "from contextlib import contextmanager\n",
    "from typing import Generator, Tuple, Iterator, Union, List, Dict\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, PretrainedConfig, AutoConfig\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from nnsight import LanguageModel\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from data.utils import get_entity_idx, get_prompts\n",
    "from mi_toolbox.utils.data_types import DataDict\n",
    "from mi_toolbox.utils.collate import TensorCollator\n",
    "from mi_toolbox.transformer_caching import caching_wrapper, decompose_attention_to_neuron, decompose_glu_to_neuron, TransformerCache\n",
    "from mi_toolbox.causal_tracing import get_mass_mean_vectors\n",
    "from mi_toolbox.contribution_tracing import get_dot_prod_contribution, get_top_x_contribution_values\n",
    "from mi_toolbox.contribution_tracing.utils import mean_center_tensor, trace_through_layer_norm\n",
    "\n",
    "DATA_DIR = \"/raid/dacslab/CONCEPT_FORMATION/homograph_small/\"\n",
    "model_ids = [\"Qwen/Qwen3-4B\"]#, \"Qwen/Qwen3-8B\", \"Qwen/Qwen3-14B\"]  # \"Qwen/Qwen3-0.6B\",\"Qwen/Qwen3-1.7B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2f87a",
   "metadata": {},
   "source": [
    "### Create parts cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8be977",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 0\n",
    "num_layers = 10\n",
    "data_dir = f'/raid/dacslab/CONCEPT_FORMATION/homograph_small/Qwen_Qwen3-4B/{sample_id}/'\n",
    "out_dir = f'/raid/dacslab/CONCEPT_FORMATION/homograph_small/Qwen_Qwen3-4B/.cache/{sample_id}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28e95c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "def cache_direct_parts(in_path, out_path, num_layers):\n",
    "\n",
    "    # load model cache\n",
    "    data_keys = ['ent_pos_idx', 'full_emb'] + [f\"{layer}.{module}\" for layer in range(num_layers) for module in ['d_attn', 'd_mlp']]\n",
    "    model_cache = TransformerCache.load(in_path, data_keys, model_id=model_ids[0])\n",
    "\n",
    "    for sample_id in range(model_cache.length):\n",
    "        \n",
    "\n",
    "    target_token_pos = model_cache['ent_pos_idx']\n",
    "    batch_ent_idx = (range(model_cache.length), target_token_pos)\n",
    "\n",
    "    # get parts for direct contributions\n",
    "    direct_parts = [model_cache['full_emb'][batch_ent_idx][:, None]]\n",
    "    del model_cache.data['full_emb']\n",
    "    gc.collect()\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        direct_parts.append(model_cache[f'{layer}.d_attn'][batch_ent_idx].flatten(1, -2))\n",
    "        del model_cache.data[f'{layer}.d_attn']\n",
    "        gc.collect()\n",
    "        \n",
    "        direct_parts.append(model_cache[f\"{layer}.d_mlp\"][batch_ent_idx])\n",
    "        del model_cache.data[f'{layer}.d_mlp']\n",
    "        gc.collect()\n",
    "        \n",
    "    direct_parts = torch.concat(direct_parts, dim=1)\n",
    "    print(direct_parts.shape)\n",
    "\n",
    "    # for i, sample_parts in enumerate(direct_parts):\n",
    "    #     os.makedirs(os.path.join(out_path, f'{i}'), exist_ok= True)\n",
    "    #     file_path = os.path.join(out_path, f'{i}/direct_parts.safetensor')\n",
    "    #     torch.save(sample_parts, file_path)\n",
    "\n",
    "cache_direct_parts(data_dir, out_dir, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afae72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "def cache_indirect_parts(in_path, out_path, num_layers):\n",
    "\n",
    "    # load model cache\n",
    "    data_keys = ['full_emb'] + [f\"{layer}.{module}\" for layer in range(num_layers) for module in ['d_attn', 'd_mlp']]\n",
    "    model_cache = TransformerCache.load(in_path, data_keys, model_id=model_ids[0])\n",
    "    model_cache.stack_tensors(padding=True)\n",
    "\n",
    "    num_samples = model_cache['full_emb'].size(0)\n",
    "    num_tokens = model_cache['full_emb'].size(1)\n",
    "\n",
    "    for sample_id in range(num_samples):\n",
    "        indirect_parts_cache = []\n",
    "        indirect_parts_cache.append(mean_center_tensor(model_cache['full_emb'])[sample_id, :, None])\n",
    "        gc.collect()\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            indirect_parts_cache.append(mean_center_tensor(model_cache[f'{layer}.d_attn'][sample_id]).flatten(1, -2))\n",
    "            gc.collect()\n",
    "\n",
    "            indirect_parts_cache.append(mean_center_tensor(model_cache[f\"{layer}.d_mlp\"][sample_id]))\n",
    "            gc.collect()\n",
    "\n",
    "        indirect_parts_cache = torch.concat(indirect_parts_cache, dim=1)\n",
    "\n",
    "        for token_pos, parts in enumerate(indirect_parts_cache):\n",
    "            os.makedirs(os.path.join(out_path, f'{sample_id}'), exist_ok= True)\n",
    "            file_path = os.path.join(out_path, f'{sample_id}/indirect_parts__pos_{token_pos}.safetensor')\n",
    "            torch.save(parts, file_path)\n",
    "\n",
    "        del indirect_parts_cache#\n",
    "        gc.collect()\n",
    "\n",
    "cache_indirect_parts(data_dir, out_dir, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "456bd114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_model_id(model_id):\n",
    "    model_id = model_id.replace('/','_')\n",
    "    return model_id\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def load_language_model(\n",
    "    model_id: Union[str, List[str]],\n",
    "    **kwargs\n",
    ") -> Generator[Tuple[PreTrainedModel, PretrainedConfig], None, None]:\n",
    "    llm = AutoModel.from_pretrained(\n",
    "        model_id,\n",
    "        **kwargs\n",
    "    )\n",
    "    try:\n",
    "        yield llm, llm.config\n",
    "    finally:\n",
    "        llm.cpu()\n",
    "        del llm\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"Memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "        \n",
    "def batch_iterator(dl: DataLoader, silent=False) -> Iterator[Dict, ]:\n",
    "    for batch_id, batch in enumerate(dl):\n",
    "        start_time = time.perf_counter()\n",
    "        try:\n",
    "            yield batch\n",
    "        finally:\n",
    "            end_time = time.perf_counter()\n",
    "            if not silent:\n",
    "                print(f\"Batch {batch_id + 1}/{len(dl)}: {(end_time - start_time):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac8a81",
   "metadata": {},
   "source": [
    "### Get direct contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc37244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BiDirectionalContributionTracer:\n",
    "\n",
    "    def __init__(self, homograph_id: int, sample_id: int, model_id: str, target_pos: str,\n",
    "                 batch_size = 16, device='cuda'):\n",
    "        self.homograph_id = homograph_id\n",
    "        self.sample_id = sample_id\n",
    "        self.model_id = model_id\n",
    "        self.target_pos = target_pos\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.num_layers = int(target_pos.split('.')[0]) + 1\n",
    "        self.num_tokens = self.load_num_token_pos()\n",
    "\n",
    "        # load model config \n",
    "        self.model_config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "        # get ent token pos\n",
    "        self.ent_token_pos = self.load_ent_token_pos()\n",
    "        self.parts_token_pos = None\n",
    "\n",
    "        self.curr_token_pos = self.ent_token_pos\n",
    "        self.curr_layer = self.num_layers - 1\n",
    "        self.curr_module = None\n",
    "\n",
    "        # module lookup\n",
    "        self.module_lookup = self.get_module_lookup()\n",
    "\n",
    "    def __enter__(self):\n",
    "        # perform caching logic\n",
    "        pass\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        # perform caching cleanup\n",
    "        pass\n",
    "\n",
    "    def load_ent_token_pos(self) -> int:\n",
    "        cache_path = f\"{DATA_DIR}/{sanitize_model_id(self.model_id)}/{self.homograph_id}\"\n",
    "        model_cache = DataDict.load(cache_path, ['ent_pos_idx'])\n",
    "\n",
    "        ent_token_pos = model_cache['ent_pos_idx'][self.sample_id]\n",
    "        return ent_token_pos\n",
    "    \n",
    "    def load_num_token_pos(self) -> int:\n",
    "        cache_path = f\"{DATA_DIR}/{sanitize_model_id(self.model_id)}/{self.homograph_id}\"\n",
    "        model_cache = TransformerCache.load(cache_path, ['0.d_attn'])\n",
    "        model_cache.stack_tensors(padding=True) # TODO due to the padding we store bigger tensors.\n",
    "        num_tokens = model_cache['0.d_attn'][self.sample_id].size(0)\n",
    "        return num_tokens\n",
    "    \n",
    "    def get_module_lookup(self) -> List[str]:\n",
    "        num_attn_heads, attn_head_dim, num_mlp_heads = self.model_config.num_attention_heads, self.model_config.head_dim, self.model_config.intermediate_size\n",
    "        module_lookup = ['0.emb.0.0']\n",
    "\n",
    "        for layer_id in range(self.num_layers):\n",
    "            module_lookup.extend([f\"{layer_id}.attn.{token_pos}.{head_pos}\" for token_pos in range(self.num_tokens) for head_pos in range(num_attn_heads * attn_head_dim)])\n",
    "            module_lookup.extend([f\"{layer_id}.mlp.0.{head_pos}\" for head_pos in range(num_mlp_heads)])\n",
    "        \n",
    "        return module_lookup\n",
    "    \n",
    "    def trace(self):\n",
    "        G, unvisited_nodes = self.init_contribution_graph()\n",
    "\n",
    "        with load_language_model(\n",
    "            model_id=model_ids[0],\n",
    "            trust_remote_code=True,\n",
    "            device_map=self.device,\n",
    "            dtype=torch.bfloat16,\n",
    "            attn_implementation = 'eager'\n",
    "        ) as (llm, config):\n",
    "            \n",
    "            projection_cache = self.load_projection_cache()\n",
    "\n",
    "            while unvisited_nodes:\n",
    "                next_module_nodes, unvisited_nodes = self.select_next_module_nodes(unvisited_nodes)\n",
    "                next_module_tasks = self.get_next_module_tasks(G, llm, projection_cache, next_module_nodes)\n",
    "\n",
    "\n",
    "                if self.parts_token_pos != self.curr_token_pos:\n",
    "                    print(f'Loading token {self.curr_token_pos} cache.')\n",
    "                    parts_cache = self.load_token_pos_parts(next_module_nodes)\n",
    "\n",
    "                contribution_end_idx = self.get_contribution_end_idx()\n",
    "\n",
    "                dl = DataLoader(next_module_tasks, batch_size=self.batch_size, collate_fn=TensorCollator(), shuffle=False)\n",
    "\n",
    "                for batch in batch_iterator(dl, silent=True):\n",
    "                    \n",
    "                    contributions = self.get_contributions(batch, parts_cache, contribution_end_idx)\n",
    "\n",
    "                    unvisited_nodes = self.update_contribution_graph(G, batch, contributions, unvisited_nodes)\n",
    "\n",
    "        return G\n",
    "    \n",
    "    def init_contribution_graph(self) -> Tuple[nx.DiGraph, List[str]]:\n",
    "        layer_id, module_type = self.target_pos.split('.')\n",
    "\n",
    "        G = nx.DiGraph()\n",
    "        G.add_node('target', token_pos=self.ent_token_pos, layer_id=int(layer_id), module_type=module_type)\n",
    "\n",
    "        return G, list(G.nodes())\n",
    "    \n",
    "    def load_projection_cache(self):\n",
    "        cache_path = f\"{DATA_DIR}/{sanitize_model_id(self.model_id)}/{self.homograph_id}\" # TODO create cache_path in __init__\n",
    "        data_keys = [f\"{layer}.{module}\" for layer in range(num_layers) for module in ['v_proj', 'attn_norm_var', 'up_proj', 'mlp_norm_var']]\n",
    "\n",
    "        projection_cache = TransformerCache.load(cache_path, data_keys, model_id=self.model_id)\n",
    "        projection_cache.stack_tensors(padding=True)\n",
    "\n",
    "        return projection_cache\n",
    "    \n",
    "    def select_next_module_nodes(self, unvisited_nodes) -> Tuple[List[str], List[str]]:\n",
    "\n",
    "        if unvisited_nodes == ['target']:\n",
    "            return unvisited_nodes, []\n",
    "\n",
    "        sorted_unvisited_nodes = sorted(unvisited_nodes, key=self.unvisited_nodes_sorter) # TODO skipp 'emb' modules\n",
    "        next_module_type = '.'.join(sorted_unvisited_nodes[-1].split('.')[:3])\n",
    "\n",
    "        next_module_nodes = []\n",
    "        while sorted_unvisited_nodes:\n",
    "            if not sorted_unvisited_nodes[-1].startswith(next_module_type): break\n",
    "            next_module_nodes.append(sorted_unvisited_nodes.pop())\n",
    "\n",
    "        return next_module_nodes, sorted_unvisited_nodes\n",
    "\n",
    "    @staticmethod\n",
    "    def unvisited_nodes_sorter(node: str) -> int:\n",
    "        layer_id, module, token_pos, _ = node.split('.')\n",
    "        return int(token_pos) * 10000 + int(layer_id) * 100 + int(module == 'mlp')\n",
    "    \n",
    "    def get_next_module_tasks(self, G: nx.DiGraph, llm: PreTrainedModel, projection_cache: TransformerCache, next_module_nodes: List[str]) -> DataDict:\n",
    "\n",
    "        if next_module_nodes == ['target']:\n",
    "\n",
    "            target_vector = self.get_target_vector()\n",
    "\n",
    "            return DataDict.from_dict({\n",
    "                \"nodes\":['target'], \n",
    "                \"target_vectors\": [target_vector],\n",
    "                \"contribution_factors\": torch.tensor([1])\n",
    "                })\n",
    "        \n",
    "        example_node = next_module_nodes[0]\n",
    "        self.update_current_module(G, example_node)\n",
    "\n",
    "        projection_vectors = self.get_projection_vectors(G, llm, projection_cache, next_module_nodes)\n",
    "        conrtibution_factors = self.get_contribution_factors(G, projection_cache, next_module_nodes)\n",
    "\n",
    "        return DataDict.from_dict({\n",
    "            \"nodes\": next_module_nodes,\n",
    "            \"target_vectors\": projection_vectors,\n",
    "            \"contribution_factors\": conrtibution_factors\n",
    "        })\n",
    "\n",
    "    def get_target_vector(self) -> torch.Tensor:\n",
    "        cache_path = f\"{DATA_DIR}/{sanitize_model_id(self.model_id)}/{self.homograph_id}\"\n",
    "        model_cache = TransformerCache.load(cache_path, [self.target_pos], model_id=self.model_id)\n",
    "        model_cache.stack_tensors()\n",
    "\n",
    "        num_samples, model_dim = model_cache[self.target_pos].shape\n",
    "        target_residual_states = model_cache[self.target_pos].view(2, num_samples // 2, model_dim).unsqueeze(0)\n",
    "        target_vector = get_mass_mean_vectors(target_residual_states)[0]\n",
    "\n",
    "        if sample_id >= num_samples / 2:\n",
    "            target_vector = -target_vector\n",
    "\n",
    "        return target_vector\n",
    "    \n",
    "    def update_current_module(self, G: nx.DiGraph, example_node: str) -> None:\n",
    "        self.curr_token_pos = G.nodes[example_node]['token_pos']\n",
    "        self.curr_layer = G.nodes[example_node]['layer_id']\n",
    "        self.curr_module = G.nodes[example_node]['module_type']\n",
    "\n",
    "    def get_projection_vectors(self, G: nx.DiGraph, llm: PreTrainedModel, projection_cache: TransformerCache, next_module_nodes: List[str]) -> torch.Tensor:\n",
    "\n",
    "        projection_heads = [G.nodes[node]['head_id'] for node in next_module_nodes]\n",
    "\n",
    "        if self.curr_module == 'attn':\n",
    "            projection_vectors = self.get_attn_projection_vectors(llm, projection_cache, projection_heads)\n",
    "            return projection_vectors\n",
    "        elif self.curr_module == 'mlp':\n",
    "            projection_vectors = self.get_mlp_projection_vectors(llm, projection_cache, projection_heads)\n",
    "            return projection_vectors\n",
    "        raise ValueError(f\"Unexpected module type: {self.curr_module}\")\n",
    "\n",
    "    def get_attn_projection_vectors(self, llm: PreTrainedModel, projection_cache: TransformerCache, projection_heads: List[int]) -> torch.Tensor:\n",
    "        num_attn_heads = self.model_config.num_attention_heads\n",
    "        num_k_v_heads = self.model_config.num_key_value_heads\n",
    "        num_head_groups = num_attn_heads // num_k_v_heads\n",
    "        head_dim = self.model_config.head_dim\n",
    "        hidden_dim = self.model_config.hidden_size\n",
    "\n",
    "        k_v_head_pos = torch.tensor(projection_heads) // head_dim // num_head_groups\n",
    "        neuron_pos = torch.tensor(projection_heads) % head_dim\n",
    "\n",
    "        v_proj_W = llm.layers[self.curr_layer].self_attn.v_proj.weight.data.view(num_k_v_heads, head_dim, hidden_dim)[(k_v_head_pos, neuron_pos)]\n",
    "        norm_W = llm.layers[self.curr_layer].input_layernorm.weight.data\n",
    "        norm_var = projection_cache[f\"{self.curr_layer}.attn_norm_var\"][(self.sample_id, self.curr_token_pos)].unsqueeze(0)\n",
    "\n",
    "        projection_vectors = trace_through_layer_norm(v_proj_W, norm_W, norm_var, device=self.device)\n",
    "\n",
    "        return projection_vectors.cpu()\n",
    "\n",
    "    def get_mlp_projection_vectors(self, llm: PreTrainedModel, projection_cache: TransformerCache, projection_heads: List[int]) -> torch.Tensor:\n",
    "        up_proj_W = llm.layers[self.curr_layer].mlp.up_proj.weight.data[projection_heads]\n",
    "        norm_W = llm.layers[self.curr_layer].post_attention_layernorm.weight.data\n",
    "        norm_var = projection_cache[f\"{self.curr_layer}.mlp_norm_var\"][(self.sample_id, self.curr_token_pos)].unsqueeze(0)\n",
    "\n",
    "        projection_vectors = trace_through_layer_norm(up_proj_W, norm_W, norm_var, device=self.device)\n",
    "\n",
    "        return projection_vectors.cpu()\n",
    "\n",
    "    def get_contribution_factors(self, G: nx.DiGraph, projection_cache: TransformerCache, next_module_nodes: List[str]) -> torch.Tensor:\n",
    "        \n",
    "        projection_heads = [G.nodes[node]['head_id'] for node in next_module_nodes]\n",
    "        contribution_values = self.get_node_contribution_values(G, next_module_nodes)\n",
    "\n",
    "        if self.curr_module == 'attn':\n",
    "            contribution_factors = self.get_attn_contribution_factors(G, projection_cache, contribution_values, projection_heads)\n",
    "            return contribution_factors\n",
    "        if self.curr_module == 'mlp':\n",
    "            contribution_factors = self.get_mlp_contribution_factors(G, projection_cache, contribution_values, projection_heads)\n",
    "            return contribution_factors\n",
    "        raise ValueError(f\"Unexpected module type: {self.curr_module}\")\n",
    "\n",
    "    def get_node_contribution_values(self, G: nx.DiGraph, next_module_nodes: List[str]) -> torch.Tensor:\n",
    "        \n",
    "        contribution_values = []\n",
    "        for projection_node in next_module_nodes:\n",
    "            incoming_edges_with_data = G.in_edges(projection_node, data=True)\n",
    "            contribution_value = sum([data['weight'] for _, _, data in incoming_edges_with_data])\n",
    "\n",
    "            G.nodes[projection_node]['contribution_value'] = contribution_value\n",
    "            contribution_values.append(contribution_value)\n",
    "\n",
    "        return torch.tensor(contribution_values)\n",
    "\n",
    "    def get_attn_contribution_factors(self, G: nx.DiGraph, projection_cache: Dict, contribution_values: torch.Tensor, projection_heads: List[int]) -> torch.Tensor:\n",
    "        num_attn_heads = self.model_config.num_attention_heads\n",
    "        num_k_v_heads = self.model_config.num_key_value_heads\n",
    "        num_head_groups = num_attn_heads // num_k_v_heads\n",
    "        head_dim = self.model_config.head_dim\n",
    "\n",
    "        k_v_head_pos = torch.tensor(projection_heads) // head_dim // num_head_groups\n",
    "        neuron_pos = torch.tensor(projection_heads) % head_dim\n",
    "\n",
    "        v_proj = projection_cache[f\"{self.curr_layer}.v_proj\"][self.sample_id, self.curr_token_pos].view(num_k_v_heads, head_dim)[k_v_head_pos, neuron_pos]\n",
    "        contribution_factors = contribution_values / v_proj\n",
    "\n",
    "        # print(sorted(list(zip(contribution_factors.tolist(), contribution_values.tolist(), v_proj.tolist())), key= lambda x: x[0], reverse=True)[:5])\n",
    "\n",
    "        return contribution_factors\n",
    "\n",
    "    def get_mlp_contribution_factors(self, G: nx.DiGraph, projection_cache: Dict, contribution_values: torch.Tensor, projection_heads: List[int]) -> torch.Tensor:\n",
    "        up_proj =  projection_cache[f\"{self.curr_layer}.up_proj\"][self.sample_id, self.curr_token_pos, projection_heads]\n",
    "        contribution_factors = contribution_values / up_proj\n",
    "\n",
    "        # print(sorted(list(zip(contribution_factors.tolist(), contribution_values.tolist(), up_proj.tolist())), key= lambda x: x[0], reverse=True)[:5])\n",
    "\n",
    "        return contribution_factors\n",
    "\n",
    "    def load_token_pos_parts(self, next_module_nodes: List[str]) -> torch.Tensor:\n",
    "\n",
    "        if next_module_nodes == ['target']:\n",
    "            parts_path = f\"{DATA_DIR}/{sanitize_model_id(self.model_id)}/.cache/{self.homograph_id}/{self.sample_id}/direct_parts.safetensor\"\n",
    "        else:\n",
    "            parts_path = f\"{DATA_DIR}/{sanitize_model_id(self.model_id)}/.cache/{self.homograph_id}/{self.sample_id}/indirect_parts__pos_{self.curr_token_pos}.safetensor\"\n",
    "\n",
    "        parts = torch.load(parts_path).unsqueeze(1)\n",
    "        self.parts_token_pos = self.curr_token_pos\n",
    "        return parts.to(self.device)\n",
    "    \n",
    "    def get_contribution_end_idx(self):\n",
    "        if self.curr_module == None:\n",
    "            return None\n",
    "        first_contribution_of_type = f\"{self.curr_layer}.{self.curr_module}.0.0\"\n",
    "        end_idx = self.module_lookup.index(first_contribution_of_type)\n",
    "        return end_idx\n",
    "\n",
    "    def get_contributions(self, batch: Dict, batch_parts: torch.Tensor, contribution_end_idx: int) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "        target_vectors = batch['target_vectors'].to(self.device)\n",
    "        contribution_factors = batch['contribution_factors'].to(self.device)\n",
    "\n",
    "        contributions = get_dot_prod_contribution(parts=batch_parts[:contribution_end_idx], whole=target_vectors).transpose(0, 1)\n",
    "        \n",
    "        scaled_contributions = contributions * contribution_factors[:, None]\n",
    "\n",
    "        top_x_contributions = get_top_x_contribution_values(scaled_contributions, 0.9) \n",
    "\n",
    "        contribution_idx = [el.nonzero(as_tuple=True)[0] for el in top_x_contributions]\n",
    "        contribution_values = [top_x_contributions[i][idx] for i, idx in enumerate(contribution_idx)]\n",
    "\n",
    "        return contribution_idx, contribution_values\n",
    "    \n",
    "    def update_contribution_graph(self, G:nx.DiGraph, batch: Dict, contributions: Tuple[torch.Tensor, torch.Tensor], unvisited_nodes: List[str]) -> List[str]:\n",
    "        source_nodes = batch['nodes']\n",
    "        unvisited_nodes = set(unvisited_nodes)\n",
    "\n",
    "        flat_contributions = self.flatten_contributions(source_nodes, contributions)\n",
    "        for source_node, idx, value in flat_contributions:\n",
    "            out_of_context_contribution_node = self.module_lookup[idx]\n",
    "\n",
    "            contextualized_contribution_node = self.contextualize_contribution_node(out_of_context_contribution_node)\n",
    "\n",
    "            if contextualized_contribution_node not in unvisited_nodes:\n",
    "                unvisited_nodes = self.add_new_contribution_node(G, unvisited_nodes, contextualized_contribution_node)\n",
    "\n",
    "            G.add_edge(source_node, contextualized_contribution_node, weight=value.item())\n",
    "\n",
    "        return list(unvisited_nodes)\n",
    "    \n",
    "    def flatten_contributions(self, source_nodes: List[str], contributions: Tuple[torch.Tensor, torch.Tensor]) -> List[Tuple[str, torch.Tensor, torch.Tensor]]:\n",
    "        flat_contributions = []\n",
    "\n",
    "        for source_node, contribution_idx, contribution_values in zip(source_nodes, *contributions):\n",
    "            for idx, value in zip(contribution_idx, contribution_values):\n",
    "                flat_contributions.append((source_node, idx, value))\n",
    "        \n",
    "        return flat_contributions\n",
    "    \n",
    "    def contextualize_contribution_node(self, contribution_node: str) -> str:\n",
    "        contribution_layer, contribution_module, contribution_pos, contribution_head = contribution_node.split('.')\n",
    "\n",
    "        if contribution_module == 'mlp' or contribution_module == 'emb':\n",
    "            contribution_pos = self.curr_token_pos\n",
    "        \n",
    "        contextualized_contribution_node = f\"{contribution_layer}.{contribution_module}.{contribution_pos}.{contribution_head}\"\n",
    "\n",
    "        return contextualized_contribution_node\n",
    "\n",
    "    \n",
    "    def add_new_contribution_node(self, G: nx.DiGraph, unvisited_nodes: List[str], contribution_node: str) -> List[str]:\n",
    "        contribution_layer, contribution_module, contribution_pos, contribution_head = contribution_node.split('.')\n",
    "\n",
    "        G.add_node(contribution_node, token_pos=int(contribution_pos), layer_id=int(contribution_layer), module_type=contribution_module, head_id=int(contribution_head))\n",
    "\n",
    "        if contribution_module != 'emb':\n",
    "            unvisited_nodes.add(contribution_node)\n",
    "\n",
    "        return unvisited_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d04a501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d96ce3aae5a4003b2bd743cab86e622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading token 3 cache.\n",
      "Loading token 2 cache.\n",
      "Loading token 1 cache.\n",
      "Loading token 0 cache.\n",
      "Memory allocated: 1.76 GB\n",
      "Memory reserved: 1.76 GB\n"
     ]
    }
   ],
   "source": [
    "tracer = BiDirectionalContributionTracer(0, 0, model_ids[0], '9.post')\n",
    "\n",
    "G = tracer.trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a68b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nx.to_pandas_edgelist(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a026f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid/dacslab/CONCEPT_FORMATION/homograph_small/Qwen_Qwen3-4B/.cache/0/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "118ac95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(project_root, f'data/contribution_cache/cache_9_post_bidir_{sample_id}.parquet')\n",
    "df.to_parquet(data_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacslab_lasse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
